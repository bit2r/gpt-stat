[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "챗GPT 확률통계",
    "section": "",
    "text": "서문",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#제23-2차-공공-빅데이터-분석활용교육-9.1819",
    "href": "index.html#제23-2차-공공-빅데이터-분석활용교육-9.1819",
    "title": "챗GPT 확률통계",
    "section": "제23-2차 공공 빅데이터 분석활용교육 (9.18~19)",
    "text": "제23-2차 공공 빅데이터 분석활용교육 (9.18~19)\n\n\n\n방위사업교육원 실습 강의",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#참고-자료",
    "href": "index.html#참고-자료",
    "title": "챗GPT 확률통계",
    "section": "참고 자료",
    "text": "참고 자료\n\n인공지능 기초\n\n데이터 과학 언플러그드 : https://r2bit.com/book_unplugged/\n리보그 세상 - 블록 언어 → R/파이썬 : https://statkclee.github.io/reeborg/\n데이터 과학 프로그래밍 : https://r2bit.com/book_programming/\n\n데이터 사이언스\n\nGS 칼텍스 디지털 아카데미: https://r2bit.com/curriculum/\n연세대학교 데이터 과학: https://statkclee.github.io/yonsei/ https://statkclee.github.io/yonsei2/\n\n챗GPT 데이터 사이언스\n\n챗GPT PPT: https://r2bit.com/bitSlide/\n챗GPT 디지털 글쓰기: https://r2bit.com/quarto/\n챗GPT 디지털 출판: https://r2bit.com/bitPublish/\n챗GPT 실습: https://r2bit.com/gpt-edu/\n챗GPT 자료: https://r2bit.com/chatGPT/\n\n커뮤니티\n\n공익법인 한국 R 사용자회: https://r2bit.com/\n서울 R 미트업: https://r2bit.com/seoul-r/\n한국 R 컨퍼런스: https://use-r.kr/",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "index.html#신간도서-챗gpt-유닉스-쉘",
    "href": "index.html#신간도서-챗gpt-유닉스-쉘",
    "title": "챗GPT 확률통계",
    "section": "[신간도서] 챗GPT 유닉스 쉘",
    "text": "[신간도서] 챗GPT 유닉스 쉘\n\n\n\n\n\n\n\n\n“챗GPT 유닉스 쉘”은 단순한 기술서적이 아니고 지금 챗GPT AI 시대를 지탱하는 사고체계다. 컴퓨터의 대부인 IBM이 새로운 제국의 시대를 열었지만, 그 제국을 흔들었던 힘은 바로 ’유닉스’였다.\n유닉스는 레고와 닮았다. 한가지 기능에 특화된 레고블록 각각은 별볼일 없지만 레고블록을 조합하면 명작으로 재탄생한다. 레고를 이해하면 유닉스도 쉽게 이해되고 강력한 소프트웨어를 제작할 수 있다.\n피터 파커(스파이더맨)의 삼촌 벤자민 파커는 “큰 힘에는 큰 책임이 따른다.”는 명언을 남겼다. 자연어를 이해하는 챗GPT가 등장하며 큰 책임을 다하면서 큰 힘을 쓸 수 있는 여건이 마련되었다.\n이 책은 21세기 가장 섹시한 직업으로 불리는 ‘데이터 과학’ 분야를 주로 다루고 있지만, 챗GPT와 유닉스의 조합이 얼마나 강력한지 직접 체험함으로써 앞으로 펼쳐질 미래를 예측하는데도 도움을 줄 것이다.\n\n📚 종이책: https://product.kyobobook.co.kr/detail/S000208801484\n📱 전자책: https://ebook-product.kyobobook.co.kr/dig/epd/ebook/E000005358063",
    "crumbs": [
      "서문"
    ]
  },
  {
    "objectID": "data_mgmt.html",
    "href": "data_mgmt.html",
    "title": "1  데이터 관리 방법론",
    "section": "",
    "text": "1.1 측정 변수의 구분\n자료가 갖는 고유한 특성을 숫자료 표현한 측정 척도에 따라, 명목형, 순서형, 구간형, 비율형 네가지로 구분된다. 측정 척도에 따라 유의미한 통계량도 함께 정해진다. (Wiener 1921) (이경화 2020; Stevens 1946)\n변동계수(Coefficient of Variation, CV)는 표준편차와 평균값의 비율로 계산되는 통계값으로 다음과 같이 정의된다. 다음에서, \\(\\sigma\\)는 표준편차이고, $$ 평균이다.\n\\[CV = \\frac{\\sigma}{\\mu} \\times 100\\%\\]\nCV는 데이터의 변동성을 상대적으로 표현할 때 주로 사용된다. 특히, 두 개 이상의 변수 변동성을 비교할 때 편리하고, 변수 단위가 다르거나 평균값이 크게 다를 경우에도 CV를 사용하면 변동성을 공정하게 비교할 수 있는 장점이 있다.\n비율척도는 0이 절대적인 의미를 갖고, 모든 사칙연산이 가능한 척도 특징을 갖고 있다. CV가 0%이면, 변동성이 전혀 없다는 것을 의미하고, CV에서 0%는 절대적인 부재를 의미하며, 이것은 비율척도의 특징 중 하나다. 따라서, CV는 비율로 표현되므로, 어떤 변수의 CV가 다른 변수 CV의 2배라면, 변동성이 2배 크다는 것을 의미한다.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#측정-변수의-구분",
    "href": "data_mgmt.html#측정-변수의-구분",
    "title": "1  데이터 관리 방법론",
    "section": "",
    "text": "명목척도(Nominal): 단순히 개체 특성 분류를 위해 숫자나 부호를 부여한 척도\n\n남자: M, 여자: F 혹은 월: 1, 화: 2, … 일:7, 혈액형: A형, B형, AB형, O형\n\n순서척도(Ordinal): 명목척도에 부가적으로 “순서(서열)” 정보가 추가된 척도\n\n군대계급: 사병, 장교, 장군 등\n\n구간척도(Interval): 순서척도에 부가적으로 “등간격” 정보가 추가된 척도\n\n구간척도는 절대 영점이 없음. 측정된 값들 간의 상대적인 차이만을 의미.\n섭씨 온도 척도에서 0도는 온도 부재를 의미하지 않음.\n덧셈과 뺄셈은 가능하지만, 곱셈과 나눗셈은 의미가 없음. 온도가 서울 10도, 제주 20도는 제주가 서울보다 온도가 2배 높다고 할 수 없음.\n온도(섭씨 20도, 화씨 68도), 시력, IQ 지수(90, 100, 110), 물가지수(기준 연도 대비 105, 110, 115) 등\n\n비율척도(Ratio): 구간척도에 “비율” 비교특성이 추가된 척도로 “비율 등간격” 특성이 포함됨.\n\n절대 영점은 해당 측정의 부재를 의미. 길이가 0cm는 길이가 없음을 의미. 무게 0kg는 무게가 없음을 의미.\n절대 ’0’을 가지고 사칙연산이 가능함. 10kg은 5kg의 2배.\n연령(20세, 30세, 40세), 월소득(2백만원, 3백만원, 4백만원`), TV 시청률 등.\n\n\n\n\n\n측정 분류",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#data-type-basics",
    "href": "data_mgmt.html#data-type-basics",
    "title": "1  데이터 관리 방법론",
    "section": "1.2 R 자료구조",
    "text": "1.2 R 자료구조\nR에서 기본으로 사용하는 벡터 자료형은 원자 벡터(Atomic Vector) 와 리스트(List) 로 나눠진다. 원자 벡터에는 6가지 자료형이 있고, logical, integer, double, character, complex, raw, 총 6 개가 있으며 주로, 논리형, 정수형, 부동소수점형, 범주형, 4가지를 많이 사용한다. (Wickham, Çetinkaya-Rundel, 와/과 Grolemund 2023)\n\n\n\n자료형(Type)\n모드(Mode)\n저장모드(Storage Mode)\n\n\n\n\nlogical\nlogical\nlogical\n\n\ninteger\nnumeric\ninteger\n\n\ndouble\nnumeric\ndouble\n\n\ncomplex\ncomplex\ncomplex\n\n\ncharacter\ncharacter\ncharacter\n\n\nraw\nraw\nraw",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#측정척도와-자료형",
    "href": "data_mgmt.html#측정척도와-자료형",
    "title": "1  데이터 관리 방법론",
    "section": "1.3 측정척도와 자료형",
    "text": "1.3 측정척도와 자료형\n측정 척도는 크게 명목척도(Nominal), 순서척도(Ordinal), 구간척도(Interval), 그리고 비율척도(Ratio)로 분류할 수 있다.\n\n명목척도(Nominal)는 순서정보가 없는 범주로, logical, character나 factor 자료형으로 표현한다.\n순서척도(Ordinal)는 순서 정보가 추가된 명목척도로 ordered factor로 표현한다.\n구간척도(Interval)에서는 덧셈과 뺄셈은 가능하지만, 곱셈과 나눗셈은 의미가 없고 절대 영점도 없다. 주로 double 또는 integer 자료형으로 표현한다.\n비율척도(Ratio)에서는 모든 사칙연산이 가능하며, 절대 ’0’을 포함된다. 주로 double 또는 integer 자료형으로 표현한다.\n\n\n\n\n\n\n\n\n\n측정 척도\n설명\nR 자료형\n\n\n\n\n명목척도(Nominal)\n숫자나 문자로 개체 특성 분류\ncharacter, factor\n\n\n순서척도(Ordinal)\n순서 정보가 추가된 명목척도\nordered factor\n\n\n구간척도(Interval)\n덧셈, 뺄셈 가능 but 곱셈, 나눗셈 의미 없음. 절대 영점 없음\ndouble, integer\n\n\n비율척도(Ratio)\n모든 사칙연산 가능. 절대 ‘0’ 포함\ndouble, integer",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#extended-data-type",
    "href": "data_mgmt.html#extended-data-type",
    "title": "1  데이터 관리 방법론",
    "section": "1.4 자료형 확장",
    "text": "1.4 자료형 확장\n범주(요인), 텍스트, 날짜와 시간도 중요한 R에서 자주 사용되는 중요한 데이터 자료형으로 별도로 다뤄진다. 이를 위해서 stringr, lubridate, forcats 팩키지를 사용해서 데이터 정제작업은 물론 기계학습 예측모형 개발에 활용한다.\n\n\n\nR 자료형\n자료형\n예제\n\n\n\n\nlogical\n부울\n부도여부(Y/N), 남여\n\n\ninteger\n정수\n코로나19 감염자수\n\n\nfactor\n범주\n정당, 색상\n\n\nnumeric\n실수\n키, 몸무게, 주가, 환율\n\n\ncharacter\n텍스트\n주소, 이름, 책제목\n\n\nDate\n날짜\n생일, 투표일\n\n\n\n\n\n\n자료형 확장",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#벡터와-리스트",
    "href": "data_mgmt.html#벡터와-리스트",
    "title": "1  데이터 관리 방법론",
    "section": "1.5 벡터와 리스트",
    "text": "1.5 벡터와 리스트\n리스트는 재귀 벡터(recursive vector)라고도 불리는데 리스트는 다른 리스트를 포함할 수 있기 때문이다.\n따라서, 원자벡터는 동질적(homogeneous)이고, 리스트는 상대적으로 이질적(heterogeneous)이다.\n모든 벡터는 두가지 성질(Property)을 갖는데, 자료형과 길이로 이를 확인하는데 typeof()와 length() 함수를 사용해서 확인한다.\n\nlibrary(tidyverse)\n\na &lt;- list(a = 1:3,\n            b = \"a string\",\n            c = pi,\n            d = list(-1, -5) )\n  \ncat(\"자료형:\", typeof(a), \" 길이: \", length(a) ) \n#&gt; 자료형: list  길이:  4",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#is-na-null",
    "href": "data_mgmt.html#is-na-null",
    "title": "1  데이터 관리 방법론",
    "section": "1.6 NULL과 NA 결측값",
    "text": "1.6 NULL과 NA 결측값\n결측되었다는 없다는 것을 표시하는 방법이 두가지 필요하다. 하나는 벡터가 없다는 NULL이고, 벡터 내부에 값이 결측되었다는 NA 다. dataframe$variable &lt;- NULL 명령문을 사용하면 데이터프레임(dataframe)에 변수(variable)를 날려보내는 효과가 있다. 예를 들어 책장이 아예 없다는 의미(NULL)와 책장에 책이 없다(NA)는 다른 개념을 지칭하고 쓰임새가 다르다.\n\n\nNULL\n\n# NULL 자료형과 길이\ntypeof(NULL)\n#&gt; [1] \"NULL\"\nlength(NULL)\n#&gt; [1] 0\n\n\n\nNA\n\n# NA 자료형과 길이\ntypeof(NA)\n#&gt; [1] \"logical\"\nlength(NA)\n#&gt; [1] 1\n\n\n\nNA의 중요한 특징은 전염된다는 것이다. 즉, NA에 연산을 가하면 연산결과는 무조건 NA가 된다. NA가 7보다 큰지, 7을 더하고 빼고, 부울 연산을 하든 NA와 연산결과는 무조건 NA가 된다.\n\nNA + 7\n#&gt; [1] NA\nNA / 7\n#&gt; [1] NA\nNA &gt; 7\n#&gt; [1] NA\n7 == NA\n#&gt; [1] NA\nNA == NA\n#&gt; [1] NA",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "data_mgmt.html#data-type-factor",
    "href": "data_mgmt.html#data-type-factor",
    "title": "1  데이터 관리 방법론",
    "section": "2.1 범주형, 순서형 자료형",
    "text": "2.1 범주형, 순서형 자료형\n범주형, 순서형 자료형을 생성하는 경우 주의를 기울여야 한다. factor 함수를 사용해서 요인형 자료형을 생성하는데, 내부적으로 저장공간을 효율적으로 사용하고 속도를 빠르게 하는데 유용한다. 순서를 갖는 범주형의 경우 factor 함수 내부에 levels 인자를 넣어 정의하면 순서 정보가 유지된다.\n\n# 범주형\nanimals_vector &lt;- c(\"Elephant\", \"Giraffe\", \"Donkey\", \"Horse\")\nfactor_animals_vector &lt;- factor(animals_vector)\nfactor_animals_vector\n#&gt; [1] Elephant Giraffe  Donkey   Horse   \n#&gt; Levels: Donkey Elephant Giraffe Horse\n\n# 순위형\ntemperature_vector &lt;- c(\"High\", \"Low\", \"High\",\"Low\", \"Medium\")\nfactor_temperature_vector &lt;- factor(temperature_vector, order = TRUE, levels = c(\"Low\", \"Medium\", \"High\"))\nfactor_temperature_vector\n#&gt; [1] High   Low    High   Low    Medium\n#&gt; Levels: Low &lt; Medium &lt; High\n\n\n# \"M\", \"F\" 수준\nsurvey_vector &lt;- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector &lt;- factor(survey_vector)\nlevels(factor_survey_vector)\n#&gt; [1] \"F\" \"M\"\n\n# \"Female\", \"Male\" 로 변환\nlevels(factor_survey_vector) &lt;- c(\"Female\", \"Male\")\nlevels(factor_survey_vector)\n#&gt; [1] \"Female\" \"Male\"\n\n\n# 문자형 벡터와 요인 벡터\nsurvey_vector &lt;- c(\"M\", \"F\", \"F\", \"M\", \"M\")\nfactor_survey_vector &lt;- factor(survey_vector)\n\n# 문자형 벡터 요약\nsummary(survey_vector)\n#&gt;    Length     Class      Mode \n#&gt;         5 character character\n\n# 요인 벡터 요약\nsummary(factor_survey_vector)\n#&gt; F M \n#&gt; 2 3\n\n\n\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of Measurement”. Science 103 (2684): 677–80. http://www.jstor.org/stable/1671815.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, 와/과 Garrett Grolemund. 2023. R for data science. \" O’Reilly Media, Inc.\".\n\n\nWiener, Norbert. 1921. “A new theory of measurement: a study in the logic of mathematics”. Proceedings of the London Mathematical Society 2 (1): 181–205.\n\n\n이경화. 2020. 고등학교 실용통계. 통계청 통계교육원.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 관리 방법론</span>"
    ]
  },
  {
    "objectID": "database_mgmt.html",
    "href": "database_mgmt.html",
    "title": "2  데이터베이스",
    "section": "",
    "text": "2.1 데이터베이스 기본소양",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "database_mgmt.html#데이터베이스-기본소양",
    "href": "database_mgmt.html#데이터베이스-기본소양",
    "title": "2  데이터베이스",
    "section": "",
    "text": "2.1.1 데이터 구분\n데이터는 정형데이터(RDBMS), 비정형 데이터, 반정형 데이터로 크게 나눌 수 있다. 각 자료형에 따라 장단점이 있는 것은 명확하다. 정형데이터는 분석하기 용이한 반면 확장성과 유연성이 떨어지고, 비정형 데이터는 분석하기는 다소 어려움이 있으나 확장성과 용이성에서는 장점을 갖는다. .csv, .xml, .json 파일은 그 중간적인 특성을 갖는 반정형 데이터로 분류도힌다.\n\n\n\n정형, 반정형, 비정형 데이터\n\n\n\n\n2.1.2 OLTP vs. OLAP\nOLTP (OnLine Transaction Processing)는 데이터 자체 처리에 중점을 둔 개념인데 반해, OLAP (OnLine Analytical Processing)은 저장된 데이터를 분석하여 꿰뚤어 볼 수 있는 능력(Insight)를 도출하는데 중심을 두고 있다.\nOLAP의 대표적인 예로 편의점 판매시점 정보관리(Point-Of-Sales, POS) 기계를 들어보자. 편의점에서 물건을 구매한 경우 다음과 같은 거래가 발생된다.\n\n고객 카드에서 현금 10,000원 인출\n편의점 통장에 10,000 지급\n명세표 출력\n\n상기 3건의 작업 프로세스가 하나의 트랜잭션(transaction) 묶어 모두 성공적으로 처리가 되어야 편의점 물건구매가 완료되도록 개발한다.\n반면에 OLAP은 데이터를 체계적으로 저장하여 데이터에 기반한 의사결정지원을 할 수 있도록 주안점을 두고 있다.\n\n\n\n\n\n\n\n\n\nOLAP\nOLTP\n\n\n\n\n목적\n트랜젝션 처리\n데이터 분석과 보고서 작성, 대쉬보드 시각화\n\n\n설계\n앱 기능 지향\n비즈니스 주제 지향\n\n\n데이터\n운영계, 실시간 최신 데이터\n정보계, 통합/이력 데이터\n\n\n크기\n기가 바이트, 스냅샷\n테라데이터, 아카이브\n\n\nSQL 쿼리\n단순 트랜잭션, 빈번한 갱신\n복잡한 집계 쿼리\n\n\n사용자\n아주 많음\n분석가 포함 일부\n\n\n\n따라서, OLTP는 운영 데이터베이스(Operational Database)로 적합하여 쓰기 업무(Write-intensive)가 많은 경우 빠르고 안전하게 레코드를 삽입(insert)하는데 특화된 반면, OLAP은 데이터 창고(Data Warehouse) 업무에 적합한데 다양한 분석업무를 수행할 때 쿼리 작업을 속도감있게 진행할 수 있어 읽기 업무(Read-intensive)에 특화되어 있다.\n\n\n2.1.3 DW, 데이터 호수\n전통적인 데이터베이스(Database)는 관계형 데이터베이스를 통해서 실시간 정형데이터를 빠르고 신뢰성있게 처리하는데 운영계를 지탱하는 주된 쓰임새가 있으며, 데이터 창고(Data Warehouse)는 이력 데이터를 통합하여 꿰뚤어 볼 수 있는 능력(Insight)을 제공함은 물론 보고서와 전체적인 현황을 대쉬보드를 통해 제공하는데 큰 의미가 있다. 데이터 호수(Data Lake)는 정형, 반정형, 비정형 데이터를 모두 저장하고 관리한다는 측면에서 유연성과 확장성을 내재하고 있으며 빅데이터를 분석하여 OLAP에서 추구하는 바를 한단계 더 넓혔다는 점에서 의의를 둘 수 있다. 1\n\n데이터 호수(Data Lake)는 특정한 구조가 없기 때문에 접근하기 용이하고 쉽게 수정하기도 용이한 반면에 데이터 창고(Data Warehouse)는 상대적으로 유연성이 떨어진다. 뿐만 아니라 데이터 과학자는 아직 결정되지 않는 비즈니스 활용 사례를 데이터 문제로 바꿔 모형을 만들고 시각화를 하는데 데이터 호수를 적합한 데 반해 비즈니스 현업전문가는 일단 전처리가 된 데이터를 데이터 창고에 넣어 특정 목적을 달성하는데 활용된다는 점에서 비교가 된다.\n\n\n\n\n\n\n\n\n\n데이터 호수\n데이터 창고\n\n\n\n\n자료구조\n원천 데이터 (Raw Data)\n전처리 된 데이터\n\n\n데이터 활용 목적\n미결정 상태\n현재 사용 중\n\n\n사용자\n데이터 과학자\n비즈니스 현업전문가\n\n\n접근성\n접근성 높고 신속한 업데이트\n변경하기 쉽지 않고 비용도 많이 소요됨.\n\n\n\n클라우드 서비스도 데이터 창고(Data Warehouse)를 기능으로 제공하고 있는데 상품명은 다음과 같다.\n\nAWS: 아마존 Redshift\nMS Azure: Azure SQL Data Warehouse\n구글: 구글 빅쿼리(Big Query)\n\n데이터 호수도 클라우드 서비스에서 제공된다. Object Storage와 함께 하둡/스파크 빅데이터 소프트웨어와 함께 검토된다.\n\nAWS: AWS S3\nMS Azure: Blob Storage / Azure Data Lake Storage\n구글: Cloud Storage\n네이버: Object Storage\n\n\n\n2.1.4 ETL과 ELT\nETL은 추출, 변환, 적재(Extract, Transform, Load)의 약자로 동일 기종 혹은 이기종의 원천데이터로부터 데이터 웨어하우스에서 쌓는 과정을 뜻하는데 변환(Transform) 과정이 무척 많은 노력이 투여된다. 반면에 ELT는 데이터를 먼저 적재한 후에 필요에 따라 변환과정을 거쳐 후속 작업에 사용한다. 데이터 호수 ELT 프로세스가 매력적으로 보이지만 데이터 카탈로그가 잘 관리되지 않는다면 데이터 늪(Data Swamp)가 될 수 있다. 2\n\n\n\nETL과 ELT 비교\n\n\n데이터 호수를 잘 관리하지 않는다면 데이터 늪에 빠질 수 있는데 메타데이터를 잘 관리하고 거버넌스를 확립해야 되고 비정형데이터도 많이 다루기 때문에 데이터 전문가 과학자를 확보하여 효율성을 높인다.\n\n\n\n데이터 호수와 늪\n\n\n\n\n2.1.5 데이터 통합(integration)\n원천데이터가 서로 다른 형태로 다양하게 존재하는 상황에서 데이터를 통합한다는 것은 시스템을 맞추는 것을 넘어 개념적인 데이터 모델로 정립하여야 하고 관련하여 파생된느 다양한 문제를 조화롭게 해결하는 것으로 정의할 수 있다.\n먼저 데이터를 한곳에 모은다고 하면 어떤 데이터를 모을 것인지 정의하고 클라우드 서비스를 사용한다고 하면 AWS Redshift 혹은 S3를 상정하고 혹시나 포함될 수 있는 개인정보도 사전에 식별하여 마스킹 등을 통해 익명화시켜야 되며 데이터 혈통(Data Lineage)도 구축하여 투명성과 가시성도 확보한다.\n\n\n\n데이터 계통도\n\n\n\n\n2.1.6 테이블 분할과 샤딩\n지금은 빅데이터 시대라 데이터가 커지게 되면 테이블에 담을 수 없는 상황이 온다. 이런 문제를 해결하기 위해 도입된 개념이 분할(Partition) 이다. 테이블 크기가 예를 들어 100GB 혹은 1TB가 되면 인덱스도 커져서 메모리에 적재가 되지 않아 쿼리 속도와 업데이트 속도가 현격히 늦어지게 된다. 이런 경우 테이블을 더 작은 단위로 쪼개는데 이를 분할(Partition)이라고 한다.\n테이블을 분할하게 될 경우 개념과 논리 데이터 모형은 동일하지만 물리 데이터 모형이 분할에 영향을 받게 된다.\n\n개념 데이터 모형(Concept Data Model): Entity, Relationship, Attributes\n\nERD(Entity Relational Diagram), UML 다이어그램\n\n논리 데이터 모형(Logical Data Model): 테이블, 칼럼, 관계\n\n데이터베이스 스키마와 모형: 관계형 모형, 스타 스키마(Star Sceman)\n분할(Partition) 혹은 샤딩(Sharding)에 영향을 받음\n\n물리 데이터 모형(Physical Data Model): 물리적 저장장치\n\n백업 시스템, 파티션, CPU, 저장공간 등\n분할(Partition) 혹은 샤딩(Sharding)의 영향을 받음.\n\n\n테이블을 분할하는 방법은 수평 분할 (Horizontal Partitioning)과 수직 분할 (Vertical Partitioning) 두가지 방법이 있다.\n\n\n\n데이터 수평 및 수직 분할\n\n\n데이터베이스 샤딩(databae sharding)은 테이블이 동일한 데이터베이스에 있지 않고 다른 기계에 있다는 점에서 차이가 난다. 3\n\n\n\n데이터베이스 샤딩(Sharding)",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "database_mgmt.html#db-design",
    "href": "database_mgmt.html#db-design",
    "title": "2  데이터베이스",
    "section": "2.2 데이터베이스 설계",
    "text": "2.2 데이터베이스 설계\n데이터베이스 설계(Database Design)는 데이터를 논리적으로 저장하는 방식으로 데이터베이스 모델(Database Model)을 사용한다. 데이터베이스 모델(Database Model)은 데이터베이스 구조에 대한 최상위 사양서의 역할을 한다. 일반적으로 관계형 데이터베이스 모형(Relational Database Model)을 사용하지만 NoSQL, 객체지향 DB 모형, 네트워크 DB 모델 등이 있다. 데이터베이스의 청사진으로 스키마를 사용해서 테이블, 필드, 관계, 인덱스, 뷰로 구성하여 작성한다.\n\n데이터 (Data) ← 논리 모형\n데이터베이스 모형 ← 데이터 구조에 대한 사양서\n스키마 ← 레고 블럭(Table, Field, Relation, Index, View 등)으로 데이터베이스 설계\n\n즉, 데이터를 체계적으로 구조화하는 논리모형을 먼저 구상하고 나서 사양서를 작성하고 실제 데이터베이스 설계로 들어간다.\n\n2.2.1 데이터 모형(Data Modeling)\n데이터를 저장하는 방식에 대해 데이터 모형(Data Model)을 제작하는 단계는 다음과 같다.\n\n개념 데이터 모형(Concept Data Model): Entity, Relationship, Attributes\n\nERD(Entity Relational Diagram), UML 다이어그램\n\n논리 데이터 모형(Logical Data Model): 테이블, 칼럼, 관계\n\n데이터베이스 스키마와 모형: 관계형 모형, 스타 스키마(Star Sceman)\n\n물리 데이터 모형(Physical Data Model): 물리적 저장장치\n\n백업 시스템, 파티션, CPU, 저장공간 등\n\n\n\n\n2.2.2 데이터 정규화 4 5\n관계형 데이터베이스의 설계에서 정규화(normalization)는 중복을 최소화하도록 데이터를 구조화하는 프로세스를 지칭한다. 관계형 모델의 발견자인 에드거 F. 커드는 1970년에 제 1 정규형(1NF)로 알려진 정규화의 개념을 도입하였고, 에드거 F. 커드는 이어서 제 2 정규형(2NF)과 제 3 정규형(3NF)을 1971년에 정의하였으며, 1974년에는 레이먼드 F. 보이스와 함께 보이스-코드 정규화(BCNF)를 정의하였다. 통상 관계형 데이터베이스 테이블이 제 3 정규(3NF)형이 되었으면 정규화(Normalization) 되었다고 한다. 따라서, 데이터 정규형(Normal Forms)은 1NF 부터 3NF까지가 많이 회자된다.\n\n제 1 정규형 (1 NF)\n\n각 레코드는 유일무이(unique)해야 한다. 즉, 중복(duplication)이 없어야 함.\n각 셀은 하나의 값만 가져야 함.\n\n제 2 정규형 (2 NF)\n\n제 1 정규형을 만족한다.\n기본키(primary key)가 한 칼럼이면 자동으로 제 2 정규형을 만족한다.\n기본키가 아닌 모든 속성이 기본키에 완전 함수 종속되어야 한다.\n\n제 3 정규형 (3 NF)\n\n제 2 정규형을 만족한다.\n기본키가 아닌 모든 속성이 기본키에 이행적 함수 종속이 되지 않아야 한다.\n즉, 이행(移行)적 함수 종속 (Transitive Functional Dependency)이 없어야 한다.\n함수 종속 사례로 X, Y, Z 에 대해 X → Y 이고 Y → Z 이면 X → Z 가 성립한다. 이를 Z 가 X 에 이행적으로 함수 종속되었다고 함.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "database_mgmt.html#dimensional-modeling",
    "href": "database_mgmt.html#dimensional-modeling",
    "title": "2  데이터베이스",
    "section": "2.3 OLAP 차원 모델링 6",
    "text": "2.3 OLAP 차원 모델링 6\n데이터베이스 모델이은 크게 탑다운(top-down) ERD 방법론과 함께 “데이터 창고(Data Warehouse)”에서 차원 모델링(DM, Dimensional Modeling)으로 나눠진다. 좀더 쉽게 이해를 하기 위해 GURU99에 나온 사례를 살펴보자. OLAP 차원 모델링은 다음 5단계로 이뤄진다.\n\n가장 먼저 비즈니스 프로세스를 하나 선택한다.\n\n마케팅, 영업, 인사 등\n\n얼마나 자세히 볼 것인지 상세화 수준(grain)을 정의한다.\n\n연도별, 분기별, 월별, 주별, 일별 등\n\n차원을 정의한다.\n\n차원은 날짜, 지점, 공장 등을 지칭한다.\n\n사실(Fact)을 정의한다.\n\n일자별 지점별 제품별 판매금액과 같이 숫자값이 된다.\n\n스키마를 제작한다.\n\n상기 정보를 바탕으로 스타 스키마(Star Schema) 혹은 눈꽃 스키마(Snowflake Schema) 를 작성한다.\n\n\n상기 작업결과 사장님이 원하는 월별 지점별 직급별 판매실적을 스타 스키마로 제작하게 된다\n\n\n2.3.1 스타 스키마\n차원 모델링을 방법론으로 스타 스키마를 채택하게 되면 차원 테이블(dimensional table)과 사실 테이블(fact table)을 정의해야 되는데 차원 테이블은 되도록이면 변하지 않는 속성(attributes)을 갖게 되는 반면, 차원 테이블은 측정값에 대한 레코드가 축적되며 자주 변경되게 되고 차원 테이블과 외래키(foreign key)로 연결된다.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "database_mgmt.html#chinook-database",
    "href": "database_mgmt.html#chinook-database",
    "title": "2  데이터베이스",
    "section": "2.4 chinook 데이터베이스",
    "text": "2.4 chinook 데이터베이스\n치눅(chinook) 데이터베이스는 Northwind 데이터베이스에서 유래되었으며, 치눅(chinooks)은 캐나다 프레리(Canadian Prairies)과 미국 대평원(Great Plains)에서 발생되는 푄 바람(fohn winds)라고 한다. lerocha/chinook-database GitHub 저장소에 다양한 데이터베이스에 사용될 수 있도록 교육용으로 잘 준비되어 있다.\n\n2.4.1 chinook ERD\n치눅 데이터베이스는 디지털 미디어 상점의 가상 데이터를 기반으로 한 공개 데이터베이스다. 치눅 데이터베이스는 앨범, 아티스트, 장르, 트랙, 재생 목록, 고객, 직원, 공급 업체를 담고 있는 테이블로 구성되어 있다.\n\nArtists: 아티스트의 이름과 정보를 포함합니다.\nAlbums: 각 앨범과 관련된 아티스트 정보를 포함합니다.\nTracks: 각 트랙의 이름, 앨범 ID, 장르 ID, 미디어 형식 및 가격과 같은 정보를 포함합니다.\nGenres: 음악의 장르(예: 록, 재즈, 팝 등) 정보를 포함합니다.\nMedia Types: 미디어 형식(예: MPEG 오디오 파일, ACC 오디오 파일 등) 정보를 포함합니다.\nPlaylists: 재생 목록과 그에 관련된 트랙 정보를 포함합니다.\nCustomers: 고객의 세부 정보와 구매 이력 정보를 포함합니다.\nInvoices & InvoiceItems: 고객의 청구서와 청구서 항목에 대한 정보를 포함합니다.\nEmployees: 직원 정보와 그들의 역할, 매니저, 고용 일자 등의 정보를 포함합니다.\nSuppliers: 공급 업체 정보를 포함합니다.\nChinook 데이터베이스는 실제 비즈니스 시나리오를 모방하여 복잡한 쿼리, 프로시저 및 다른 데이터베이스 작업을 수행하기 위한 교육용 플랫폼으로 사용됩니다.\n\n각 테이블간의 관계는 chinook ERD를 통해서 YugaByte DB에서 확인할 수 있다.\n\n\n\n치눅 데이터베이스 ERD\n\n\n\n\n2.4.2 헬로 월드\n먼저, sqlite3 chinooks 데이터베이스를 다운로드 한다. 7\n\ndownload.file(url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\", destfile = \"data/Chinook_Sqlite.sql\")\n\n쉘에서 아래 명령어로 sqlite3 데이터베이스를 생성시킨다. 제법 시간이 걸리기 때문에 chinook.db 생성이 완료된 후에 후속 분석 작업을 수행한다.\n$ sqlite3 chinook.db &lt; Chinook_Sqlite.sql\nDBI 연결시켜 chinook.db를 R에서 작업할 수 있도록 준비시킨다.\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(RSQLite)\n\nchinook &lt;- dbConnect(RSQLite::SQLite(), dbname = \"data/chinook.db\")\n\nR 마크다운에서 제대로 되는지 테스트 한다.\n\nSELECT * FROM Artist LIMIT 10;\n\n\nDisplaying records 1 - 10\n\n\nArtistId\nName\n\n\n\n\n1\nAC/DC\n\n\n2\nAccept\n\n\n3\nAerosmith\n\n\n4\nAlanis Morissette\n\n\n5\nAlice In Chains\n\n\n6\nAntônio Carlos Jobim\n\n\n7\nApocalyptica\n\n\n8\nAudioslave\n\n\n9\nBackBeat\n\n\n10\nBilly Cobham\n\n\n\n\n\nSQLite에서는 모든 테이블, 인덱스, 트리거 및 뷰에 대한 메타데이터를 저장하는 특별한 sqlite_master 테이블이 존재한다. SQLite 데이터베이스에서 테이블 목록을 다음 쿼리를 사용하여 확인한다.\n\ndbGetQuery(chinook, '\nSELECT name, type \n         FROM sqlite_master \n        WHERE type IN (\"table\", \"view\")\n')\n#&gt;             name  type\n#&gt; 1          Album table\n#&gt; 2         Artist table\n#&gt; 3       Customer table\n#&gt; 4       Employee table\n#&gt; 5          Genre table\n#&gt; 6        Invoice table\n#&gt; 7    InvoiceLine table\n#&gt; 8      MediaType table\n#&gt; 9       Playlist table\n#&gt; 10 PlaylistTrack table\n#&gt; 11         Track table\n\n미국에서 가장 많이 팔리는 장르는 무엇인가? 라는 질의문에 대한 SQL 쿼리를 다음과 같이 작성할 수 있다. 다음 SQL 쿼리는 SQLite in R - Answering Questions with Data에서 가져왔다.\n\ntracks_usa &lt;- '\nWITH usa_tracks_sold AS\n   (\n    SELECT il.* \n      FROM InvoiceLine  il\n     INNER JOIN Invoice i on i.InvoiceId = il.InvoiceId\n     INNER JOIN Customer c on c.CustomerId = i.CustomerId\n     WHERE c.Country = \"USA\"\n   )\nSELECT g.name AS genre_name,\n       COUNT(uts.InvoiceLineId) AS no_of_tracks_sold,\n       CAST(COUNT(uts.InvoiceLineId) AS FLOAT) / (\n                                                    SELECT COUNT(*) \n                                                      FROM usa_tracks_sold\n                                                    ) AS percentage_sold\n  FROM usa_tracks_sold uts\n INNER JOIN Track  t on t.TrackId = uts.TrackId\n INNER JOIN Genre  g on g.GenreId = t.GenreId\n GROUP BY 1\n ORDER BY 2 DESC\n LIMIT 10;\n'\n\ndbGetQuery(chinook, tracks_usa)\n#&gt;            genre_name no_of_tracks_sold percentage_sold\n#&gt; 1                Rock               157      0.31781377\n#&gt; 2               Latin                91      0.18421053\n#&gt; 3               Metal                64      0.12955466\n#&gt; 4  Alternative & Punk                50      0.10121457\n#&gt; 5                Jazz                22      0.04453441\n#&gt; 6               Blues                15      0.03036437\n#&gt; 7            TV Shows                14      0.02834008\n#&gt; 8            R&B/Soul                12      0.02429150\n#&gt; 9              Comedy                 8      0.01619433\n#&gt; 10          Classical                 8      0.01619433\n\n챗GPT를 사용하여 다음과 같이 질의를 하게 되면 SQL 쿼리문을 생성해준다.\n\n프롬프트: 매출 상위 10개 국가 목록을 뽑아주세요.\n\n\ndbGetQuery(chinook, '\n  SELECT \n      c.Country,\n      SUM(il.UnitPrice * il.Quantity) AS TotalRevenue\n  FROM Invoice i\n  JOIN InvoiceLine il ON i.InvoiceId = il.InvoiceId\n  JOIN Customer c ON i.CustomerId = c.CustomerId\n  GROUP BY c.Country\n  ORDER BY TotalRevenue DESC\n  LIMIT 10;\n')\n#&gt;           Country TotalRevenue\n#&gt; 1             USA       523.06\n#&gt; 2          Canada       303.96\n#&gt; 3          France       195.10\n#&gt; 4          Brazil       190.10\n#&gt; 5         Germany       156.48\n#&gt; 6  United Kingdom       112.86\n#&gt; 7  Czech Republic        90.24\n#&gt; 8        Portugal        77.24\n#&gt; 9           India        75.26\n#&gt; 10          Chile        46.62",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "database_mgmt.html#footnotes",
    "href": "database_mgmt.html#footnotes",
    "title": "2  데이터베이스",
    "section": "",
    "text": "talend, ““Four key differences between a data lake and a data warehouse↩︎\nthenortonsetup (Jan 10, 2017), “Don’t Let Your Data Lake Turn Into A Data Swamp”↩︎\nCUBRID SHARD, “Database Sharding”↩︎\n위키 백과, “데이터베이스 정규화”↩︎\nYABOONG (MARCH 9, 2018), “데이터베이스 정규화 - 1NF, 2NF, 3NF”↩︎\nGuru99, “What is Dimensional Modeling in Data Warehouse?”↩︎\nJohn Atten (2014-12-07), “Installing and Using SQLite on Windows”↩︎",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>데이터베이스</span>"
    ]
  },
  {
    "objectID": "document.html",
    "href": "document.html",
    "title": "3  문서",
    "section": "",
    "text": "3.1 논문과 ropensci 진행경과\n(Marwick, Boettiger, 와/과 Mullen 2018)",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#papers-ropensci",
    "href": "document.html#papers-ropensci",
    "title": "3  문서",
    "section": "",
    "text": "재현가능한 과학연구를 위한 저작 방법\n\n\n\nropensci, “Community Call: Reproducible Research with R”",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#compendium-intro",
    "href": "document.html#compendium-intro",
    "title": "3  문서",
    "section": "3.2 R 팩키지와 개요서",
    "text": "3.2 R 팩키지와 개요서\nR 프로젝트, R 팩키지, 개요서(compendium)가 서로 동일한 목표를 가지고 있지만, 다소 차이가 있는 것도 사실이다. 데이터 사이언스를 하면서 코드, 데이터, 결과물을 하나의 개요서(compendium) 아래 묶어 이를 통해 재현가능한 과학기술 발전을 도모하는 것이 무엇보다 필요하다. 3 4\n\n좋은 데이터 사이언스 프로젝트 구성\n\n모든 파일은 동일한 디렉토리 아래 정돈되어 있음\n원본 데이터(raw data)는 별도 폴더에 잘 저장되어 있어야 함.\n정제된 데이터는 R 스크립트를 통해서 만들어져야 함.\n함수는 분석 스크립트와 독립되어 저장되어야 함.\n함수는 문서화가 잘 되어야 하면 (단위) 테스트도 되어 ㅎ함.\n산출물은 코드와 격리되어야 하며 일회용으로 한번 사용하고 버림.\nMakefile은 적절한 순서로 분석을 실행해야 함.\nREADME 파일은 프로젝트 개요를 담고 있어야 함.\ngit을 사용해서 R 코드와 Rmd 문서 파일은 버전 제어를 해야 함.\n\n\nproject/\n|-+ data-raw/   # 원본 데이터\n|-+ data/       # (R 스크립트로부터 생성된) 정제된 데이터\n|-+ R/          # 함수(Functions)\n|-+ man/        # (Roxygen으로 생성된) 함수 문서(Function documentation)\n|-+ tests/      # 테스트 (functions, Rmd)\n|-+ vignettes/  # (Rmd로 작성된) 분석결과, 원고, 보고서 등\n|\n|- Makefile    # 자동화 시키는 마스터 스크립트\n|- DESCRIPTION # 메타데이터와 의존성\n|- README      # 프로젝트 개요서\n상기와 같이 개요서를 R 팩키지를 통해 구현하게 되면 어떤 점이 좋은지 살펴보자.\n\n재현가능성: Reproducibility\n일관되고, 표준적이며, 물흐르는 듯한 프로젝트 구조: Consistent, standard, streamlined organisation\n모듈화, 문서화, 테스트 주도 철학을 증진: Promotes modular, well-documented and tested code\n공유하기 쉬움: Easy to share (zip, GitHub repo)\n설치와 실행이 쉬움: Easy to install & run (Dependencies)\nR 팩키지 제작 기계: Use R package development machinery:\nR CMD CHECK\n지속 개발/지속 배포: Continuous integration (Travis-CI)\ngoodpractice로 자동화된 코드리뷰: Automatic code review with goodpractice\npkgdown으로 프로젝트 웹사이트 제작: Easily create project websites with pkgdown",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#start-small",
    "href": "document.html#start-small",
    "title": "3  문서",
    "section": "3.3 시작이 반이다",
    "text": "3.3 시작이 반이다\n\n시작이 반이다 - start small\n\nproject\n|- DESCRIPTION\n|- README.md  \n|- Metadata.txt\n|\n|- data/                \n|   +- 2014ParasiteSurveyJustBrood.csv\n|   +- CedarBPLifeTable2014.csv\n|   +- NorthBPLifeTable2013.txt\n|   +- NorthBPLifeTable2014.csv|\n|- analysis/\n|   +- CodeforBPpaper.R",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#pkg-development",
    "href": "document.html#pkg-development",
    "title": "3  문서",
    "section": "3.4 팩키지 개발",
    "text": "3.4 팩키지 개발\n\n팩키지 개발\n\n\n\nproject\n|- DESCRIPTION\n|- NAMESPACE\n|- README.md\n|- LakeTrophicModelling.Rproj\n|\n|- R/\n|   +- LakeTrophicModelling-package.r\n|   +- class_prob_rf.R\n|   +- condAccuracy.R\n|   +- crossval_rf.R\n|   +- density_plot.R\n|   +- ecdf_ks_ci.R\n|   +- ecor_map.R\n|   +- getCyanoAbund.R\n|   +- getLakeIDs.R\n|   +- importancePlot.R\n|\n|- man/\n|   +- class_prob_rf.Rd\n|   +- condAccuracy.Rd\n|   +- crossval_rf.Rd\n|   +- density_plot.Rd\n|   +- ecdf_ks_ci.Rd\n|   +- ecor_map.Rd\n|   +- getCyanoAbund.Rd\n|   +- getLakeIDs.Rd\n|   +- importancePlot.Rd\n|\n|- data/                \n|   +- LakeTrophicModelling.rda\n|\n|- vignettes/\n|\n|- inst/\n|   +- doc/\n|      +- manuscript.pdf\n|   +- extdata/\n|      +- ltmData.csv\n|      +- data_def.csv",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#cicd-docker",
    "href": "document.html#cicd-docker",
    "title": "3  문서",
    "section": "3.5 CI/CD와 도커",
    "text": "3.5 CI/CD와 도커\nDockerfile 파일을 추가하여 환경도 재현가능하게 만들 수 있고, .travis.yml을 추가하여 CI/CD 환경도 구축할 수 있다. tests/를 추가하여 테스트 주도 개발(test-driven development, TDD)를 시도할 수 있고, 이를 통해 수작업 검증을 자동화하여 생산성과 품질을 대폭 향상시킬 수도 있다.\nproject\n|- DESCRIPTION          # project metadata and dependencies \n|- README.md            # top-level description of content and guide to users\n|- NAMESPACE            # exports R functions in the package for repeated use\n|- LICENSE              # specify the conditions of use and reuse of the code, data & text\n|- .travis.yml          # continuous integration service hook for auto-testing at each commit\n|- Dockerfile           # makes a custom isolated computational environment for the project\n|\n|- data/                # raw data, not changed once created\n|  +- my_data.csv       # data files in open formats such as TXT, CSV, TSV, etc.\n|\n|- analysis/            # any programmatic code\n|  +- my_report.Rmd     # R markdown file with narrative text interwoven with code chunks \n|  +- makefile          # builds a PDF/HTML/DOCX file from the Rmd, code, and data files\n|  +- scripts/          # code files (R, shell, etc.) used for data cleaning, analysis and visualisation \n|\n|- R/                     \n|  +- my_functions.R    # custom R functions that are used more than once throughout the project\n|\n|- man/\n|  +- my_functions.Rd   # documentation for the R functions (auto-generated when using devtools)\n|\n|- tests/\n|  +- testthat.R        # unit tests of R functions to ensure they perform as expected",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#case-by-case-template",
    "href": "document.html#case-by-case-template",
    "title": "3  문서",
    "section": "3.6 각 사례별 템플릿",
    "text": "3.6 각 사례별 템플릿\n\nJeff Hollister’s manuscriptPackage\nCarl Boettiger’s template\nFrancisco Rodriguez-Sanchez’s template\nBen Marwick’s researchcompendium\n\nrrtools: Tools for Writing Reproducible Research in R\n\n\n\n\n\n\nMarwick, Ben, Carl Boettiger, 와/과 Lincoln Mullen. 2018. “Packaging data analytical work reproducibly using R (and friends)”. The American Statistician 72 (1): 80–88.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document.html#footnotes",
    "href": "document.html#footnotes",
    "title": "3  문서",
    "section": "",
    "text": "“Statistical Analyses and Reproducible Research” Bioconductor Project Working Papers↩︎\n“Reproducible Research: A Bioinformatics Case Study” in Statistical Applications in Genetics and Molecular Biology↩︎\njennybc, “Use of an R package to facilitate reproducible research”↩︎\nFrancisco Rodriguez-Sanchez “Structuring data anlaysis projects as R packages”↩︎",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>문서</span>"
    ]
  },
  {
    "objectID": "document_auto.html",
    "href": "document_auto.html",
    "title": "4  보고서 자동화",
    "section": "",
    "text": "4.1 보고서 작성 작업흐름\n일반적인 보고서 작성을 위한 작업흐름과 필요한 요소는 다음과 같다. 데이터 기반 보고서를 작성할 때 데이터와 데이터를 처리하는데 필요한 R/SAS/SPSS/파이썬 스크립트, 마크다운으로 작성한 문서가 포함된다. 즉, 데이터를 데이터베이스, 웹, 파일형태로 가져오면 R 스크립트를 통해 전처리를 하고 필요한 통계량을 뽑아내고 시각적으로 히스토그램, 막대그래프, 원그래프, 시계열 추세 선 그래프 등이 포함된다. 그리고, 데이터에서 나온 다양한 통계량 및 시각적 산출물에 대한 견해와 함께 최종 보고서 작성자의 의견을 덧붙여 마무리하고 보고서 작성자, 참고문헌, 목차, 각주 등을 붙여 보고서를 완성한다.\nGUI가 아닌 CLI 방식으로 데이터를 분석한 후에 이를 .R 파일로 저장하고 나서 이를 .Rmd 파일(코드와 문서)로 함께 작성한 뒤에 이를 기계에 던져 원하는 형태의 문서 .pdf, .html, .docx 파일을 뽑아내서 웹, PC 배포문서, 저작 가능한 문서 형태로 배포한다.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>보고서 자동화</span>"
    ]
  },
  {
    "objectID": "document_auto.html#보고서-작성-작업흐름",
    "href": "document_auto.html#보고서-작성-작업흐름",
    "title": "4  보고서 자동화",
    "section": "",
    "text": "일별 통계보고서 자동화",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>보고서 자동화</span>"
    ]
  },
  {
    "objectID": "document_auto.html#다수-보고서-자동-생성",
    "href": "document_auto.html#다수-보고서-자동-생성",
    "title": "4  보고서 자동화",
    "section": "4.2 다수 보고서 자동 생성",
    "text": "4.2 다수 보고서 자동 생성\n하루일과는 아침에 일어나서 밥을 먹고, 학교나 회사에 출근하고, 점심먹고, 오후에 놀거나 추가 작업을 하고, 퇴근을 하고 저녁을 먹고, 쉬고 잠을 자는 과정이 일반적인 일상이다. 이런 과정이 매일 매일 반복된다. 마찬가지로 보고서도 이런 일상적인 과정을 담아내야 한다. 즉, 일별로 생성되는 데이터를 동일한 형태의 보고서로 작성하게 된다. 이를 위해 상기 “일반적인 보고서 작성 흐름”을 복사해서 붙여넣기 신공을 발휘하기 보다는 데이터를 바꿔넣고 해당 데이터를 매개변수(parameter)로 던져서 일별 보고서를 자동화시킨다.\n따라서, 일자 정보를 .Rmd 파일에 전달하는 과정을 거치는 것이 필요하고, 통계보고서를 자동생성시키는 과정을 별도 스크립트로 만들어서 실행시킨다.\n\n\n\n주간/월간 통계보고서 자동화",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>보고서 자동화</span>"
    ]
  },
  {
    "objectID": "document_auto.html#보고서-생성-자동화-r",
    "href": "document_auto.html#보고서-생성-자동화-r",
    "title": "4  보고서 자동화",
    "section": "4.3 보고서 생성 자동화 (R)",
    "text": "4.3 보고서 생성 자동화 (R)\n보고서 생성 자동화 프로세스를 구현하는 방법은 다양하다. RStudio 통합개발도구를 바탕으로 운영체제 쉘로 내려가지 않고 R 스크립트 내에서 작업하는 업무 구현사례는 다음과 같다.\n즉, 다양한 보고서 템프릿을 생성시켜놓는다. 일별, 주별, 월별, 년도별 보고서를 템플릿형태로 작성하고 나서 내용을 채워 넣는다.\n보고서 내용보다 변하는 것이 데이터가 된다. 2017-01-01 데이터부터 일별로 데이터가 쭉 생성된다. 이를 일자별로, 주별, 월별, 연도별로 인식한 후 이에 대한 적절한 데이터 분석결과를 담아내는 분석 R 스크립트를 활용한다.\n데이터, 보고서 콘텐츠, 데이터분석 스크립트가 모두 준비되면 이를 리포트보고서(.Rmd)가 담고 있어야 한다. 마지막으로 make_report.r을 실행하여 보고서를 자동생성 시킨다.\n\n\n\n주간/월간 통계보고서 자동화 구현사례\n\n\n\n4.3.1 보고서 Make 파일\n일자별로 보고서를 생성시키기 위해서 보고서에 일자를 .Rmd 파일에 넘겨야 한다. 우선 일자 정보를 보고서에 전달할 수 있는 형태로 만들기 위해서 다음과정을 거친다.\n\n리포트 생성일자를 자동 생성\n월, 일을 뽑아내고 0을 덧붙여 일자를 두자리로 생성\n해당 리포트는 daily_cars_report.Rmd 파일에 params 리스트로 월(dmonth)과 일(dday)을 넘김.\n출력결과는 html_document 형식으로 지정\n한국어가 깨지는 문제가 있어 인코딩은 UTF-8으로 필히 지정\n1월1일부터 5월18일까지 일별로 돌리게 되면 오류가 생겨는 경우 이를 무시하고 계속 보고서 생성시키도록 try, silent=TRUE를 지정하여 넘김.\n\n\nlibrary(stringr)\nlibrary(lubridate)\n\nreporting_date &lt;- seq(as.Date(\"2017-01-01\"), as.Date(\"2017-05-18\"), by = \"day\")\n\n\nfor(i in reporting_date) {\n    ## 날짜뽑아내기\n    xmonth &lt;- month(as.Date(i, origin=\"1970-01-01\"))\n    xday &lt;- day(as.Date(i, origin=\"1970-01-01\"))\n    dmonth &lt;- stringr::str_pad(xmonth, 2, pad=\"0\")\n    dday &lt;- stringr::str_pad(xday, 2, pad=\"0\")\n    ## 보고서 생성\n    try(\n        rmarkdown::render('./report/daily_cars_report.Rmd',\n                      params = list(\n                          dmonth = dmonth,\n                          dday   = dday),\n                      output_format=\"html_document\",\n                      output_file =paste0 (\"daily_cars_2017\", dmonth, dday, \".html\"),\n                      encoding = 'UTF-8'\n        ),\n    silent = TRUE)\n}\n\n\n\n4.3.2 매개변수를 받는 보고서\nmake_report.r 파일에서 정의된 매개변수(parameter)를 받는 .Rmd 파일은 다음과 같은 모양이 된다. 즉, YAML 헤더에 params에 dmonth, dday를 정의하고 이를 params$dmonth, params$dday로 문서의 변수로 적용한다. 이를 통해 일자별로 생성되는 데이터를 자동으로 분석하여 보고서를 자동생성시키게 된다.\n\n---\ntitle: 보고서 예제\ndate: '`r strftime(Sys.time(), format = \"%B %d, %Y\")`'\noutput:\n  html_document:\n    toc: yes\n    toc_depth: 2\nparams:\n    dmonth:\n        value: x\n    dday:\n        value: x\n---\n\n#```{r, include = F}\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\nlibrary(tidyverse)\n\noptions(scipen = 999, stringsAsFactors = FALSE)\n#```\n\n### 3.3. 보고서 시작합니다.\n\n#```{r, sample-report}\nprint(paste(params$directory, params$file, sep = '/'))\ngetwd()\ndataset &lt;- read.csv(paste0(\"../data/cars_2017\", params$dmonth, params$dday, \".rds\"))\nglimpse(dataset)\n#```",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>보고서 자동화</span>"
    ]
  },
  {
    "objectID": "document_auto.html#예측모형-보고서",
    "href": "document_auto.html#예측모형-보고서",
    "title": "4  보고서 자동화",
    "section": "4.4 예측모형 보고서",
    "text": "4.4 예측모형 보고서\nR 스크립트로 데이터 분석과정을 프로그래밍하게 되면, 이를 다양한 R 팩키지와 조합하여 예측모형을 담은 보고서도 손쉽게 작성할 수 있다.\n\n\n\n예측모형 보고서 자동화\n\n\n\n데이터\n\n원본작업데이터: 로그파일, 엑셀파일, RDBMS 등\nR 바이너리 파일: 아스키 형태 데이터(.csv 등)로 저장할 경우 메타 정보가 증발하여 R 작업흐름에 태워서 사용할 경우 .rds와 같은 R 바이너리 파일이 유용하다.\n보고서 HTML 파일: 최종 산출물 보고서\n\nR 스크립트\n\n원본 데이터를 정제하는 R 스크립트\n보고서 혹은 예측모형을 산출하는 R 스크립트\n\n\n\n4.4.1 원본데이터 정제\n윈도우 터미널을 열고 RScript 명령어를 실행한다.\n\nR 정제 파일: cleansing_code.R\n데이터 디렉토리: ../data\n채팅 로그 파일: abc.txt\n처리결과 저장 디렉토리: ../data_processed\n처리결과 파일명: abc_clean.rds\n\n\nC:\\xwMOOC_project\\code&gt; RScript cleansing_code.R \"../data\" \"abc.txt\" \"../data_processed\"\n\n[1] \"../data_processed\\\\abc_clean.rds\"\n\n\n\n4.4.2 보고서 생성\n정제된 데이터(.rds)를 통계분석하여 예측모형이 포함된 보고서를 생성한다.\n\nR make 파일: make_report.R\n보고서 Rmd 파일: report_automation.Rmd 은 make_report.R 파일 내 지정됨\n정제된 데이터 디렉토리: data_processed\n정제된 데이터 파일: abc_clean.rds\n\n\nC:\\xwMOOC_project\\report&gt; RScript make_report.R \"data_processed\" \"abc_clean.rds\"\n\nprocessing file: report_automation.Rmd\noutput file: report_automation.knit.md\n\nOutput created: report_abc_clean_file.html",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>보고서 자동화</span>"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "5  모형 관리",
    "section": "",
    "text": "5.1 모형 카드\n모형 카드(“Model Card”)는 기계 학습 모델의 성능, 사용 사례, 제한 사항 및 이상적인 운영 조건 등의 중요한 세부 사항을 기술하는 문서를 지칭한다. Model Card의 주요 목적은 기계학습 모형 사용자에게 모형 특성과 제한 사항을 명확하게 알려주어, 모형이 적절하게 사용할 수 있도록 돕는 것이다.\n모형 카드(Model Card)는 기계 학습 모델의 다양한 측면을 설명하기 위해 다음 정보를 포함하고 있다.",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>모형 관리</span>"
    ]
  },
  {
    "objectID": "model.html#모형-카드",
    "href": "model.html#모형-카드",
    "title": "5  모형 관리",
    "section": "",
    "text": "모델 정보(Model Information)\n\n모델 이름\n버전\n사용 사례\n모델 아키텍처\n학습 알고리즘\n라이센스 정보 등\n\n데이터셋 정보(Dataset Information)\n\n학습 데이터셋 설명\n검증 데이터셋 설명\n테스트 데이터셋 설명\n데이터 수집 방법\n데이터 처리 방법 등\n\n모델 성능(Metrics)\n\n전체 성능 지표\n하위그룹별 성능 지표\n가능한 경우, 편향(bias) 분석 결과\n\n편향 및 공정성 평가(Bias & Fairness Evaluation)\n\n고려된 편향 요소\n평가 방법\n결과 및 해석\n\n사용 상황 및 제한 사항(Use-case & Limitations)\n\n추천되는 사용 사례\n추천되지 않는 사용 사례\n알려진 제한 사항\n\n모델 관리 및 업데이트(Model Management)\n\n업데이트 빈도\n업데이트 메커니즘\n모델의 사용 기간\n\n추가 정보(Additional Information)\n\n연구 논문 또는 참조 자료\n문의처\n관련 웹사이트 링크 등",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>모형 관리</span>"
    ]
  },
  {
    "objectID": "model.html#펭귄-성별예측",
    "href": "model.html#펭귄-성별예측",
    "title": "5  모형 관리",
    "section": "5.2 펭귄 성별예측",
    "text": "5.2 펭귄 성별예측\n파머펭귄 데이터셋에서 결측값을 제거하고 기계학습모형을 훈련, 시험 데이터로 나누고 모형성능을 평가한 후 펭귄 암수성별 예측 모형을 배포한다. Code Interpreter를 사용해서 코드를 작성한다.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the dataset\npenguins = pd.read_csv('data/penguins.csv')\n\n# Preprocessing\npenguins['bill_length_mm'].fillna(penguins['bill_length_mm'].median(), inplace=True)\npenguins['bill_depth_mm'].fillna(penguins['bill_depth_mm'].median(), inplace=True)\npenguins['flipper_length_mm'].fillna(penguins['flipper_length_mm'].median(), inplace=True)\npenguins['body_mass_g'].fillna(penguins['body_mass_g'].median(), inplace=True)\npenguins = penguins.dropna(subset=['sex'])\n\n# Splitting the data into training and testing sets\nX = penguins.drop(columns=['rowid', 'sex'])\ny = penguins['sex']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# One-hot encoding categorical variables\nencoder = OneHotEncoder(drop='first')\nencoded_train = encoder.fit_transform(X_train[['species', 'island']])\nencoded_test = encoder.transform(X_test[['species', 'island']])\nX_train_encoded = pd.concat([X_train.drop(columns=['species', 'island']), pd.DataFrame(encoded_train.toarray(), index=X_train.index)], axis=1)\nX_test_encoded = pd.concat([X_test.drop(columns=['species', 'island']), pd.DataFrame(encoded_test.toarray(), index=X_test.index)], axis=1)\n\n# 데이터 프레임의 모든 열 이름을 문자열로 변환\nX_train_encoded.columns = X_train_encoded.columns.astype(str)\nX_test_encoded.columns = X_test_encoded.columns.astype(str)\n\n\n# Training a logistic regression model\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train_encoded, y_train)\n\n# Predictions\ny_pred = clf.predict(X_test_encoded)\n\n# Model evaluation\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\nprint(\"Classification Report:\", classification_rep)\n\nAccuracy: 0.8955223880597015\nClassification Report:               precision    recall  f1-score   support\n\n      female       0.94      0.86      0.90        36\n        male       0.85      0.94      0.89        31\n\n    accuracy                           0.90        67\n   macro avg       0.90      0.90      0.90        67\nweighted avg       0.90      0.90      0.90        67",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>모형 관리</span>"
    ]
  },
  {
    "objectID": "model.html#성별예측-모형카드",
    "href": "model.html#성별예측-모형카드",
    "title": "5  모형 관리",
    "section": "5.3 성별예측 모형카드",
    "text": "5.3 성별예측 모형카드\n펭귄 암수성별 예측 모형에 대한 모델 카드를 다음과 같이 작성할 수 있다. 모형 카드는 펭귄 성별 예측 모델의 기본 정보와 성능, 사용 상황 및 제한 사항 등을 포함하고 있고, 필요한 경우 추가적인 정보나 세부 사항을 포함시킬 수 있다.\n\n\n5.3.1 Model Card: 펭귄 성별 예측 모델\n\n모델 정보(Model Information)\n\n모델 이름: 펭귄 성별 예측 모델\n버전: 1.0\n사용 사례: 펭귄의 특성(부리 크기, 날개 길이, 몸무게 등)을 기반으로 성별을 예측\n모델 아키텍처: 로지스틱 회귀\n학습 알고리즘: 로지스틱 회귀 학습 알고리즘\n라이센스 정보: Open Source\n\n데이터셋 정보(Dataset Information)\n\n데이터셋 출처: 제공되지 않음\n특성: species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n대상 변수: sex (male, female)\n\n모델 성능(Metrics)\n\n정확도(Accuracy): 89.5%\n여성 펭귄 정밀도(Precision for Female): 94%\n여성 펭귄 재현율(Recall for Female): 86%\n남성 펭귄 정밀도(Precision for Male): 85%\n남성 펭귄 재현율(Recall for Male): 94%\n\n편향 및 공정성 평가(Bias & Fairness Evaluation)\n\n해당 정보가 제공되지 않았으므로, 해당 항목은 생략합니다.\n\n사용 상황 및 제한 사항(Use-case & Limitations)\n\n추천되는 사용 사례: 연구 목적, 펭귄의 성별을 빠르게 예측\n추천되지 않는 사용 사례: 실제 환경에서의 중요한 의사 결정\n알려진 제한 사항: 제공된 데이터셋에만 최적화됨\n\n모델 관리 및 업데이트(Model Management)\n\n업데이트 빈도: 없음 (첫 버전)\n업데이트 메커니즘: 모델을 다시 훈련하여 업데이트\n모델의 사용 기간: 데이터가 변하지 않는 한 계속 사용 가능\n\n추가 정보(Additional Information)\n\n연구 논문 또는 참조 자료: https://arxiv.org/abs/1810.03993\n문의처: admin@r2bit.com\n관련 웹사이트 링크: https://r2bit.com/",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>모형 관리</span>"
    ]
  },
  {
    "objectID": "model_value.html",
    "href": "model_value.html",
    "title": "6  예측모형 가치",
    "section": "",
    "text": "6.1 데이터 준비\n먼저 UCI 웹사이트에서 데이터를 다운로드 받아 압축을 풀고, 예측모형에 사용될 수 있도록 데이터를 가공한다. 약 11.3% 고객이 은행상품에 가입한 것이 파악된다.\n# 0. 환경설정 -----\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(modelplotr) # devtools::install_github(\"modelplot/modelplotr\")\n\n# 1. 데이터 다운로드 -----\ndownload.file(url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\", destfile = \"data//bank-additional.zip\", mode='wb')\n\n# 1.1. 압축풀기 -----\nunzip(\"data/bank-additional.zip\", exdir=\"./data\")\n\n# 1.2. 불러오기 -----\nbank_dat &lt;- read_delim(\"data/bank-additional/bank-additional-full.csv\", delim=\";\",\n                      col_types = cols(\n                              .default = col_character(),\n                              age = col_integer(),\n                              duration = col_integer(),\n                              campaign = col_integer(),\n                              pdays = col_integer(),\n                              previous = col_integer(),\n                              emp.var.rate = col_double(),\n                              cons.price.idx = col_double(),\n                              cons.conf.idx = col_double(),\n                              euribor3m = col_double(),\n                              nr.employed = col_double()))\n\n\nbank_df &lt;- bank_dat %&gt;% \n    select_('y','duration','campaign','pdays','previous','euribor3m')\n\n\nbank_df &lt;- bank_df %&gt;% \n    mutate(y = factor(y, levels=c('no', 'yes')))\n\nbank_df %&gt;% \n    count(y) %&gt;% \n    mutate(pcnt = scales::percent(n /sum(n)))",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret",
    "href": "model_value.html#predictive-model-caret",
    "title": "6  예측모형 가치",
    "section": "6.2 예측모형",
    "text": "6.2 예측모형\ncaret 팩키지를 통해서 예측모형을 개발한다. createDataPartition() 함수로 훈련/시험 데이터를 나누고, CV 방법을 통해서 최적의 모형을 개발하도록 doSNOW 팩키지로 멀티코어를 활용한 병렬처리를 가능하도록 해서 RPART, GLM, RF 모형에 따른 최적 모형을 개발한다.\n\n# 2. 예측모형 -----\n## 2.1. 훈련/시험 데이터 분할 ------\nlibrary(caret)\n\nbank_index &lt;- createDataPartition(bank_df$y, times =1, p=0.3, list=FALSE)\n\ntrain_df &lt;- bank_df[bank_index, ]\ntest_df  &lt;- bank_df[-bank_index, ]\n\n## 2.2. 모형 개발/검증 데이터셋 준비 ------\n\ncv_folds &lt;- createMultiFolds(train_df$y, k = 10, times = 3)\n\ncv_cntrl &lt;- trainControl(method = \"repeatedcv\", number = 10,\n                         repeats = 3, index = cv_folds)\n\n\n## 2.2. 모형 개발/검증 데이터셋 준비 ------\n\nlibrary(doSNOW)\n# 실행시간\nstart.time &lt;- Sys.time()\n\ncl &lt;- makeCluster(4, type = \"SOCK\")\nregisterDoSNOW(cl)\n\nbank_rpart &lt;- train(y ~ ., data = train_df, \n                    method = \"rpart\", \n                    trControl = cv_cntrl, \n                    tuneLength = 7)\n\nbank_glm   &lt;- train(y ~ ., data = train_df, \n                    method = \"glm\",\n                    family = \"binomial\",\n                    trControl = cv_cntrl, \n                    tuneLength = 7)\n\nbank_rf    &lt;- train(y ~ ., data = train_df, \n                   method = \"rf\",\n                   trControl = cv_cntrl, \n                   tuneLength = 7,\n                   importance = TRUE)\n\nstopCluster(cl)\n\ntotal.time &lt;- Sys.time() - start.time\ntotal.time\n\n# bank_rpart_m &lt;- bank_rpart$finalModel\n# bank_glm_m &lt;- bank_glm$finalModel",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-dataset",
    "href": "model_value.html#predictive-model-caret-business-dataset",
    "title": "6  예측모형 가치",
    "section": "7.1 이득과 향상도 데이터",
    "text": "7.1 이득과 향상도 데이터\n이득(gain), 향상도(lift) 계산을 위해서 먼저 예측모형에서 데이터를 준비한다. 필요한 데이터는 예측값(확률/스코어 점수)과 라벨이 된다.\n\nbank_rpart_pred &lt;- predict(bank_rpart, newdata=test_df, type=\"prob\")[,2] %&gt;% tbl_df\nbank_glm_pred   &lt;- predict(bank_glm, newdata=test_df, type=\"prob\")[,2] %&gt;% tbl_df\nbank_rf_pred    &lt;- predict(bank_rf, newdata=test_df, type=\"prob\")[,2] %&gt;% tbl_df\n\nbank_pred_df &lt;- data.frame(bank_glm_pred, bank_rpart_pred, bank_rf_pred, test_df$y) %&gt;% tbl_df %&gt;% \n    rename(prob_glm =value,\n           prob_rpart = `value.1`, \n           prob_rf = `value.2`, \n           y = `test_df.y`)\n\nbank_pred_df %&gt;% \n    sample_n(100) %&gt;% \n    DT::datatable()",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-dataset-compute",
    "href": "model_value.html#predictive-model-caret-business-dataset-compute",
    "title": "6  예측모형 가치",
    "section": "7.2 이득과 향상도 계산",
    "text": "7.2 이득과 향상도 계산\n예측모형에서 이득과 향상도 계산을 위한 데이터가 준비되었다면 다음 단계로 이득(gain), 향상도(lift)를 계산하여 데이터프레임으로 준비한다.\n\n7.2.1 caret 팩키지 - lift() 함수\ncaret 팩키지 lift() 함수를 통해 이득과 향상도를 계산할 수 있으나 총 표본 대비 비율로 나눠져서 사용시 주의가 요망된다.\n\nlift_df &lt;- caret::lift(y ~ prob_glm + prob_rf + prob_rpart, data=bank_pred_df, cuts=11, class=\"yes\")\n\nlift_df$data %&gt;% \n    filter(liftModelVar == 'prob_rf')  %&gt;% \n    DT::datatable()\n\n# ggplot(lift_df, value=10)\n\n\n\n7.2.2 gains 팩키지 - gains() 함수\ngains 팩키지 gains() 함수를 통해 이득과 향상도를 계산할 수 있으나, 그 다음 후속 작업을 위해서 데이터프레임 변환작업을 수행하면 되는데 기본적으로 향상도와 이득에 대한 중요정보를 십분위수에 맞춰 모두 계산되어 있어 이를 참조값으로 사용한다.\n\nlibrary(gains)\n\ngains_tbl &lt;- gains(actual    = as.integer(bank_pred_df$y)-1,\n                   predicted = bank_pred_df$prob_glm, \n                   groups=10)\n\ngains_df &lt;- tibble(\n    x = gains_tbl$depth,\n    obs = gains_tbl$obs,\n    cume.obs = gains_tbl$cume.obs,\n    mean.resp = gains_tbl$mean.resp, \n    cume.mean.resp = gains_tbl$cume.mean.resp,\n    cume.pct.of.total = gains_tbl$cume.pct.of.total,\n    lift = gains_tbl$lift, \n    cume.lift = gains_tbl$cume.lift,\n    mean.prediction = gains_tbl$mean.prediction,\n    min.prediction = gains_tbl$mean.prediction, \n    max.prediction = gains_tbl$max.prediction, \n    conf = gains_tbl$conf, \n    optimal = gains_tbl$optimal, \n    num.groups = gains_tbl$num.groups,\n    percents = gains_tbl$percents\n) %&gt;% \n    add_row(x=0, obs=0, cume.obs=0, mean.resp=0, cume.mean.resp=0, cume.pct.of.total=0, lift=0,\n            cume.lift=0, mean.prediction=0, min.prediction=0, max.prediction=0, conf=\"none\",\n            optimal = \"false\",num.groups=10, percents=\"false\") %&gt;% \n    arrange(x)\n\nDT::datatable(gains_df)\n\n# gains_df %&gt;% \n#     ggplot(aes(x=x, y=cume.pct.of.total)) +\n#       geom_line() +\n#       geom_point() +\n#       scale_y_continuous(limits=c(0,1)) +\n#       geom_abline(slope=1, intercept = 0)\n\n\n\n7.2.3 사용자 정의 함수\nLISTEN DATA, “UNDERSTAND GAIN AND LIFT CHARTS”를 참조해서 직접 사용자 정의 함수를 작성해도 가능하다.\n\ncustom_lift &lt;- function(label_var, prob_var, groups=10) {\n    \n    tmp_df &lt;- data.frame(cbind(label_var, prob_var))\n    \n    tmp_df &lt;- tmp_df %&gt;%\n        mutate(decile = ntile(desc(prob_var), groups))\n\n    gain_table &lt;- tmp_df %&gt;% \n        group_by(decile) %&gt;%\n        summarise_at(vars(label_var), funs(total = n(),\n                                           total_resp = sum(., na.rm = TRUE))) %&gt;%\n        mutate(cum_resp = cumsum(total_resp),\n               gain     = cum_resp / sum(total_resp) * 100,\n               cum_lift = gain / (decile*(100/groups)))\n    return(gain_table)\n}\n\ncustom_lift_df &lt;- custom_lift(as.integer(bank_pred_df$y)-1, bank_pred_df$prob_glm, 10)\n\nDT::datatable(custom_lift_df)",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-cumulative-gains",
    "href": "model_value.html#predictive-model-caret-business-cumulative-gains",
    "title": "6  예측모형 가치",
    "section": "7.3 누적 이득 그래프",
    "text": "7.3 누적 이득 그래프\n누적 이득 그래프(Cumulative gains plot)는 예측모형을 적용해서 십분위 X 까지 선택하게 되면, 실제 목표 라벨을 실제 몇 % 까지 예측/기대할 수 있는지에 대한 답을 제공한다.\nRF가 최종 예측모형으로 선정되었다면 이를 활용하여 “누적 이득 그래프”로 시각화해보고 예측모형의 사업성과를 추정해보자.\n\nlibrary(extrafont)\nloadfonts()\n\n(gains_glm_tbl &lt;- gains(actual    = as.integer(bank_pred_df$y)-1,\n                       predicted = bank_pred_df$prob_glm, \n                       groups=10))\n\ngains_glm_df &lt;- tibble(\n    x = gains_glm_tbl$depth,\n    obs = gains_glm_tbl$obs,\n    cume.obs = gains_glm_tbl$cume.obs,\n    mean.resp = gains_glm_tbl$mean.resp, \n    cume.mean.resp = gains_glm_tbl$cume.mean.resp,\n    cume.pct.of.total = gains_glm_tbl$cume.pct.of.total,\n    lift = gains_glm_tbl$lift, \n    cume.lift = gains_glm_tbl$cume.lift,\n    mean.prediction = gains_glm_tbl$mean.prediction,\n    min.prediction = gains_glm_tbl$mean.prediction, \n    max.prediction = gains_glm_tbl$max.prediction, \n    conf = gains_glm_tbl$conf, \n    optimal = gains_glm_tbl$optimal, \n    num.groups = gains_glm_tbl$num.groups,\n    percents = gains_glm_tbl$percents\n) %&gt;% \n    add_row(x=0, obs=0, cume.obs=0, mean.resp=0, cume.mean.resp=0, cume.pct.of.total=0, lift=0,\n            cume.lift=0, mean.prediction=0, min.prediction=0, max.prediction=0, conf=\"none\",\n            optimal = \"false\",num.groups=10, percents=\"false\") %&gt;% \n    arrange(x) %&gt;% \n    mutate(decile = x / 10 %&gt;% as.integer)\n\n\n\ngains_glm_df %&gt;% \n    ggplot(aes(x=decile, y=cume.pct.of.total)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_abline(slope=0.1, intercept = 0) +\n      scale_x_continuous(limits=c(0,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(labels = scales::percent) +\n      theme_minimal() +\n      labs(x=\"십분위\", y=\"누적 이득(Cumulative Gains)\", title=\"포르투칼 마케팅 캠페인 누적 이득 그래프\") \n\n\n\n\n포르투칼 마케팅 캠페인 누적 이득 그래프",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-lift",
    "href": "model_value.html#predictive-model-caret-business-lift",
    "title": "6  예측모형 가치",
    "section": "7.4 향상 리프트(Lift) 그래프",
    "text": "7.4 향상 리프트(Lift) 그래프\n누적 향상(리프트) 그래프(Cumulative lift plot)는 예측모형을 적용해서 십분위수 X까지 선택하게 되면, 예측모형을 전혀 사용하지 않을 때 대비하여 몇배나 더 효과가 있는지에 대한 대답을 제공한다.\n\ngains_glm_df %&gt;% \n    filter(x !=0) %&gt;% \n    ggplot(aes(x=decile, y=cume.lift/100)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_hline(yintercept=1, size=1.3, color=\"darkgray\") +\n      scale_x_continuous(limits=c(1,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(limits=c(0.5,6), breaks = seq(0,6,1)) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"십분위\", y=\"누적 향상도(Cumulative Lift)\", title=\"포르투칼 마케팅 캠페인 누적 향상도 그래프\") \n\n\n\n\n포르투칼 마케팅 캠페인 누적 향상도 그래프",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-response-plot",
    "href": "model_value.html#predictive-model-caret-business-response-plot",
    "title": "6  예측모형 가치",
    "section": "7.5 반응 그래프(Reponse Plot)",
    "text": "7.5 반응 그래프(Reponse Plot)\n반응 그래프(Response Plot)는 예측모형을 적합시켜서 십분위수 X를 선택하게 되면, 해당 십분위수에서 기대되는 예상 반응율을 몇 %가 되는지에 대한 답을 제공한다.\n\navg_response_pcnt &lt;- gains_glm_df$cume.mean.resp[11]\n\ngains_glm_df %&gt;% \n    filter(x !=0) %&gt;% \n    ggplot(aes(x=decile, y=mean.resp)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_hline(yintercept=avg_response_pcnt, size=1.3, color=\"darkgray\") +\n      scale_x_continuous(limits=c(1,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(limits=c(0,0.6), breaks = seq(0,0.6,0.1), labels = scales::percent) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"십분위\", y=\"평균 반응율(%)\", title=\"포르투칼 마케팅 캠페인 반응율 그래프\") \n\n\n\n\n포르투칼 마케팅 캠페인 반응율 그래프",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#predictive-model-caret-business-cumulative-response-plot",
    "href": "model_value.html#predictive-model-caret-business-cumulative-response-plot",
    "title": "6  예측모형 가치",
    "section": "7.6 누적 반응 그래프(Cumulative Reponse Plot)",
    "text": "7.6 누적 반응 그래프(Cumulative Reponse Plot)\n누적 반응 그래프(Cumulative Response Plot)는 예측모형을 적합시켜서 해당 십분위수 X까지 누적하여 선택하게 되면, 해당 십분위수에서 기대되는 예상 반응율을 몇 %가 되는지에 대한 답을 제공한다.\n\ngains_glm_df %&gt;% \n    filter(x !=0) %&gt;% \n    ggplot(aes(x=decile, y=cume.mean.resp)) +\n      geom_point(size=2) +\n      geom_line(size=1.3) +\n      geom_hline(yintercept=avg_response_pcnt, size=1.3, color=\"darkgray\") +\n      scale_x_continuous(limits=c(1,10), breaks = seq(0,10,1)) +\n      scale_y_continuous(limits=c(0,0.6), breaks = seq(0,0.6,0.1), labels = scales::percent) +\n      theme_minimal(base_family = \"NanumGothic\") +\n      labs(x=\"십분위\", y=\"누적 평균 반응율(%)\", title=\"포르투칼 마케팅 캠페인 누적 반응율 그래프\") \n\n\n\n\n포르투칼 마케팅 캠페인 누적 반응율 그래프",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "model_value.html#footnotes",
    "href": "model_value.html#footnotes",
    "title": "6  예측모형 가치",
    "section": "",
    "text": "Bank Marketing Data Set, The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y)↩︎\nJurriaan Nagelkerke and Pieter Marcus (2018-09-20), “Introducing modelplotr: Plots to evaluate the business value of predictive models”↩︎\nCumulative Gain Chart↩︎\nDnI institute, “Model Validation using R- German Credit Data”↩︎\nLISTEN DATA, “UNDERSTAND GAIN AND LIFT CHARTS”↩︎",
    "crumbs": [
      "데이터 관리",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>예측모형 가치</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html",
    "href": "cs_nightingale.html",
    "title": "7  나이팅게일",
    "section": "",
    "text": "7.1 배경\n크림 전쟁은 1853년부터 1856년까지 일어난 큰 전쟁이었다. 한쪽에는 러시아, 반면 다른 한쪽에는 영국, 프랑스, 오스만 제국 (현대 투르키에), 그리고 나중에 사르디니아 (현대 이탈리아의 일부)가 동맹을 구성하여 전쟁을 치뤘다. 전쟁이 바로 시작된 이유는 러시아가 오스만 제국 내 정교회 신자들을 보호하려 하려는 명분을 내세웠지만, 사실 더많은 영토를 차지하기 위함이였다. 양측간 전쟁은 흑해를 두고 남하하는 러시아에 맞서 동맹군이 크림반도에서 발생하여 “크림전쟁”(Crimean War)으로 불린다. 영화로 소개된 경기병대의 돌격 (“Charge of the Light Brigade”), 영국 간호사 플로렌스 나이팅게일의 활약, 전신과 철도의 본격적인 도입으로 큰 의미를 갖는 전쟁이기도 하다. 많은 전투와 많은 사람들이 죽은 후, 1856년 파리 조약으로 전쟁은 마무리되어, 러시아 확장은 잠시 멈추게 돼었고, 오스만 제국도 한숨 돌린 계기가 되었다.\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원(Scutari Hospital, Turkey)은 영국 군 병원으로 개조되었다. 크림전쟁에서 부상을 당한 수많은 병사가 치료를 위해 이곳으로 보내졌지만, 병자와 부상병들을 감당할 수 있도록 설계되지 않았고 제대로된 역할도 수행하지 못했다. 1854년 나이팅게일이 간호사 일행과 함께 도착했을 때, 비위생적인 환경과 고통받는 병사들을 보고 경악했다. 나이팅게일의 스쿠타리 병원에서 경험은 병원과 의료 서비스를 개선하여 이와 같은 고통과 비극이 재발하지 않도록 향후 프로젝트의 중요한 동기와 방향이 되었다.\n환자의 사망율을 42%에서 2%로 낮추고 집중치료실(ICU)을 설치하여 상태가 중한 환자를 격리하여 집중관리하는 등 근대적인 간호체계를 수립하는 데 기여하였다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#배경",
    "href": "cs_nightingale.html#배경",
    "title": "7  나이팅게일",
    "section": "",
    "text": "스쿠타리 병원의 한 병동 석판화 그림 (William Simpson)",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#원본-데이터",
    "href": "cs_nightingale.html#원본-데이터",
    "title": "7  나이팅게일",
    "section": "7.2 원본 데이터",
    "text": "7.2 원본 데이터\n크림 전쟁 중 스쿠타리 막사는 투루키에 스쿠타리 병원에서 몇년간에 걸쳐 수작업으로 종이에 분석가능한 형태의 자료를 만들어내는 것은 결코 쉬운 작업이 아니다.\n\n\n\n원본 데이터",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#그래프-진화",
    "href": "cs_nightingale.html#그래프-진화",
    "title": "7  나이팅게일",
    "section": "7.3 그래프 진화",
    "text": "7.3 그래프 진화\n출처: How Florence Nightingale Changed Data Visualization Forever - The celebrated nurse improved public health through her groundbreaking use of graphic storytelling\n복잡한 논거를 제시하는 대신 구체적인 주장에 데이터 시각화와 데이터 스토리텔링(Storytelling)을 통해 청중에 한걸음 더 다가섰다. 나이팅게일의 스토리텔링은 열악한 위생 상태와 과밀로 인해 불필요한 죽음이 얼마나 많이 발생하는지 이해하기 쉬운 비교를 통해 이야기를 구성해서 설득해 나갔다. 예를 들어, 군대 사망률을 민간인 사망률(유사한 환경의 맨체스터)과 비교하는 프레임을 제시하고, 군대 막사에서 생활하는 평시 병사들이 비슷한 연령대 민간인 남성보다 더 높은 비율로 사망하는 것을 제시했다. 이를 통해, 데이터가 보여주는 현실을 부정할 수 없게 만들었고, 군대 행정에 극적인 개혁을 이끌어냈다.\n\n\n\n\n\n\n\n\n\n\n\n(a) 막대그래프\n\n\n\n\n\n\n\n\n\n\n\n(b) 맨체스터 사망\n\n\n\n\n\n\n\n\n\n\n\n(c) 빅토리아 여왕 보고(I)\n\n\n\n\n\n\n\n\n\n\n\n(d) 빅토리아 여왕 보고(II)\n\n\n\n\n\n\n\n\n\n\n\n(e) 빅토리아 여왕 보고(III)\n\n\n\n\n\n\n\n그림 7.1: 나이팅게일 그래프 진화과정",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#설득",
    "href": "cs_nightingale.html#설득",
    "title": "7  나이팅게일",
    "section": "7.4 설득",
    "text": "7.4 설득\n나이팅게일은 크림 전쟁 중 병원에서의 위생 문제와 관련된 데이터를 수집하고 분석하여 그 결과를 시각화했고, 병원에서의 사망 원인 중 대부분이 감염성 질병으로 인한 것을 발견했다. 이러한 감염성 질병은 부적절한 위생 조건과 밀접한 관련이 있음을 확인했다.\n나이팅게일은 병원의 위생 상태를 개선을 통해 수많은 생명을 구할 수 있다는 사실을 확인했고 연구결과와 권장 사항을 다양한 영국 정부부처에 제출했고, 특히 1858년에 영국의 장관들에게 보고서를 제출했다. 이를 통해서 군 병원의 위생 조건을 개선하는 데 큰 영향을 미쳤다.\n\n\n\n나이팅게일과 빅토리아 여왕",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#성과와-영향",
    "href": "cs_nightingale.html#성과와-영향",
    "title": "7  나이팅게일",
    "section": "7.5 성과와 영향",
    "text": "7.5 성과와 영향\n나이팅게일 캠페인이 민간 공중보건에 미친 가장 큰 영향은 실현되기까지 오랜 기간에 걸쳐 다각도로 검토되었고, 마침내 1875년 영국 공중보건법(British Public Health Act)에 법제화되었다. 이 법에는 잘 정비된 하수도, 깨끗한 수돗물, 건축법 규제 등의 요건이 담겨있다. 질병에 대한 면역력을 강화하는 백신과 농작물 수확량을 획기적으로 늘리는 인공비료 개발과 함께 이 제도적인 노력으로 평균 수명을 두 배로 늘리는 원동력이 되었다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#작업과정",
    "href": "cs_nightingale.html#작업과정",
    "title": "7  나이팅게일",
    "section": "7.6 작업과정",
    "text": "7.6 작업과정\n\n7.6.1 디지털 데이터\nrladies/spain_nightingale GitHub 저장소에서 엑셀 형태로 된 데이터를 가져와서 전처리할 수 있다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\ndeath_raw &lt;- read_excel(\"data/datos_florence.xlsx\", sheet = \"Sheet1\", skip = 1)\n\ndeath_tbl &lt;- death_raw |&gt; \n  janitor::clean_names() |&gt; \n  set_names(c(\"Month\", \"Army\", \"Disease\", \"Wounds\", \"Other\", \"Disease.rate\", \"Wounds.rate\", \"Other.rate\")) |&gt; \n  mutate(Date = lubridate::my(Month)) |&gt; \n  separate(Month, into = c(\"Month\", \"Year\"), sep = \" |_\") |&gt; \n  select(Date, Month, Year, everything()) \n\ndeath_tbl\n#&gt; # A tibble: 24 × 10\n#&gt;    Date       Month Year   Army Disease Wounds Other Disease.rate Wounds.rate\n#&gt;    &lt;date&gt;     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 1854-04-01 Apr   1854   8571       1      0     5          1.4         0  \n#&gt;  2 1854-05-01 May   1854  23333      12      0     9          6.2         0  \n#&gt;  3 1854-06-01 Jun   1854  28333      11      0     6          4.7         0  \n#&gt;  4 1854-07-01 Jul   1854  28722     359      0    23        150           0  \n#&gt;  5 1854-08-01 Aug   1854  30246     828      1    30        328.          0.4\n#&gt;  6 1854-09-01 Sep   1854  30290     788     81    70        312.         32.1\n#&gt;  7 1854-10-01 Oct   1854  30643     503    132   128        197          51.7\n#&gt;  8 1854-11-01 Nov   1854  29736     844    287   106        341.        116. \n#&gt;  9 1854-12-01 Dec   1854  32779    1725    114   131        632.         41.7\n#&gt; 10 1855-01-01 Jan   1855  32393    2761     83   324       1023.         30.7\n#&gt; # ℹ 14 more rows\n#&gt; # ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\nHistDate 패키지에 동일한 데이터셋이 잘 정제되어 있어 이를 바로 활용해도 좋다.\n\nlibrary(HistData)\n\nHistData::Nightingale |&gt; \n  as_tibble()\n#&gt; # A tibble: 24 × 10\n#&gt;    Date       Month  Year  Army Disease Wounds Other Disease.rate Wounds.rate\n#&gt;    &lt;date&gt;     &lt;ord&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt;  1 1854-04-01 Apr    1854  8571       1      0     5          1.4         0  \n#&gt;  2 1854-05-01 May    1854 23333      12      0     9          6.2         0  \n#&gt;  3 1854-06-01 Jun    1854 28333      11      0     6          4.7         0  \n#&gt;  4 1854-07-01 Jul    1854 28722     359      0    23        150           0  \n#&gt;  5 1854-08-01 Aug    1854 30246     828      1    30        328.          0.4\n#&gt;  6 1854-09-01 Sep    1854 30290     788     81    70        312.         32.1\n#&gt;  7 1854-10-01 Oct    1854 30643     503    132   128        197          51.7\n#&gt;  8 1854-11-01 Nov    1854 29736     844    287   106        341.        116. \n#&gt;  9 1854-12-01 Dec    1854 32779    1725    114   131        632.         41.7\n#&gt; 10 1855-01-01 Jan    1855 32393    2761     83   324       1023.         30.7\n#&gt; # ℹ 14 more rows\n#&gt; # ℹ 1 more variable: Other.rate &lt;dbl&gt;\n\n\n\n7.6.2 데이터와 사투\n앞서 준비한 death_tbl 데이터프레임에서 사망 관련 데이터를 처리하고 시각화하기 위한 전처리를 수행하여 시각화를 위한 준비작업을 수행한다. 먼저 Date, Disease.rate, Wounds.rate, Other.rate 칼럼을 선택하고, pivot_longer 함수를 사용해 시각화에 적합한 데이터로 재구조화한다. str_replace_all 함수를 사용하여 칼럼 이름에서 “.rate”를 제거하고, ifelse 함수를 이용해 날짜를 기준으로 나이팅게일 팀이 준비한 방식을 적용하기 전과후 “이전”과 “이후”로 체제로 구분한다. factor 함수를 사용하여 범주 순서를 정의하고, 마지막으로 month 함수를 이용해 날짜에서 해당 월을 추출하고 death_viz에 저장한다.\n\ndeath_viz &lt;- death_tbl %&gt;% \n  select(Date, Disease.rate, Wounds.rate, Other.rate) %&gt;% \n  pivot_longer(-Date, names_to = \"사망원인\", values_to = \"사망자수\") |&gt; \n  mutate(사망원인 = str_replace_all(사망원인, \"\\\\.rate\", \"\"), \n         체제 = ifelse(Date &lt;= as.Date(\"1855-03-01\"), \"조치이전\", \"조치이후\")) %&gt;% \n  mutate(체제 = factor(체제, levels = c(\"조치이전\", \"조치이후\"))) %&gt;%  \n  mutate(해당월 = month(Date, label = TRUE, abbr = TRUE)) |&gt; \n  mutate(사망원인 = case_when(사망원인 == \"Disease\" ~ \"질병\",\n                              사망원인 == \"Wounds\" ~ \"부상\",\n                              사망원인 == \"Other\" ~ \"기타\")) |&gt; \n  mutate(사망원인 = factor(사망원인, levels = c(\"질병\", \"부상\", \"기타\")))\n\ndeath_viz\n#&gt; # A tibble: 72 × 5\n#&gt;    Date       사망원인 사망자수 체제     해당월\n#&gt;    &lt;date&gt;     &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;    &lt;ord&gt; \n#&gt;  1 1854-04-01 질병          1.4 조치이전 \" 4\"  \n#&gt;  2 1854-04-01 부상          0   조치이전 \" 4\"  \n#&gt;  3 1854-04-01 기타          7   조치이전 \" 4\"  \n#&gt;  4 1854-05-01 질병          6.2 조치이전 \" 5\"  \n#&gt;  5 1854-05-01 부상          0   조치이전 \" 5\"  \n#&gt;  6 1854-05-01 기타          4.6 조치이전 \" 5\"  \n#&gt;  7 1854-06-01 질병          4.7 조치이전 \" 6\"  \n#&gt;  8 1854-06-01 부상          0   조치이전 \" 6\"  \n#&gt;  9 1854-06-01 기타          2.5 조치이전 \" 6\"  \n#&gt; 10 1854-07-01 질병        150   조치이전 \" 7\"  \n#&gt; # ℹ 62 more rows",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#시각화",
    "href": "cs_nightingale.html#시각화",
    "title": "7  나이팅게일",
    "section": "7.7 시각화",
    "text": "7.7 시각화\n‘ggplot2’ 패키지를 이용하여 크림전쟁 나이팅게일 활약상을 담은 데이터를 시각화한다. 나이팅게일 활약 전과 후로 데이터(death_viz)를 나눠 “크림전쟁 병사 사망원인”에 대한 극좌표계 시각화를 통해 이해하기 쉬운 설득력있는 시각화 결과물을 제시하고 있다. 추가적으로, ‘showtext’ 패키지로 구글 “Noto Serif KR” 글꼴을 선택적용하고, ‘hrbrthemes’ 라이브러리를 이용하여 뒷 배경 검정색을 사용하여 붉은색 질병으로 인한 사망자수 확연한 감소를 시각적으로 강조한다.\n\nlibrary(hrbrthemes) \nlibrary(showtext)\nshowtext.auto()\nfont_add_google(name = \"Noto Serif KR\", family = \"noto_serif\")\nnoto_font &lt;- \"noto_serif\"\n\ndeath_gg &lt;- death_viz %&gt;% \n  ggplot(aes(x = 해당월, y = 사망자수, fill = 사망원인)) +\n  geom_col(color = \"grey20\") + \n  theme_modern_rc(base_family = noto_font, subtitle_family = noto_font) + \n  scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\"), name = \"\") +\n  scale_y_sqrt() +\n  facet_wrap(~ 체제) + \n  coord_equal(ratio = 1) +  \n  coord_polar() +\n  labs(title = \"크림전쟁 병사 사망원인\", \n       subtitle = \"데이터 시각화와 커뮤니케이션\", \n       caption = \"데이터 출처: 크림전쟁 사망자\") + \n  theme(legend.position = \"top\", \n        text = element_text(family = noto_font, size = 18),\n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        plot.margin = unit(rep(0.7, 4), \"cm\"),\n        plot.title = element_text(color = \"white\", family = noto_font, size = 25),\n        plot.caption = element_text(color = \"grey70\", family = noto_font, size = 12),\n        plot.subtitle = element_text(color = \"grey70\", size = 13),\n        legend.text = element_text(color = \"white\", size = 15),\n        strip.text = element_text(color = \"white\", size = 25, face = \"bold\", family = noto_font, hjust = 0.5))\n\ndeath_gg\n\nragg::agg_jpeg(\"images/death_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\ndeath_gg\ndev.off()\n\n\n\n7.7.1 선그래프\n나이팅게일은 간호 분야의 선구자로 잘 알려져 있지만, 통계학자로서 “콕스콤(CoxComb)” 또는 “장미 다이어그램”(Rose Diagram)으로 알려진 원그래프를 제시하였지만 현재는 시간의 흐름에 따라 병사 사망자수 변화를 조치 전후로 명확히 하는 방법으로 선그래프가 기본 기법으로 자리잡고 있다.\n\n# extrafont::loadfonts()\n\ndeath_new_gg &lt;- death_viz |&gt; \n  ggplot(aes(x = Date, y = 사망자수, color = 사망원인)) +\n    geom_line() +\n    geom_point() +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font)          \n\ndeath_new_gg\n\nragg::agg_jpeg(\"images/death_new_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\ndeath_new_gg\ndev.off()\n\n\n\n\n7.7.2 막대그래프\n동일한 정보를 막대그래프를 통해 시각화를 할 수도 있다. 원그래프와 비교하여 보면 명확하게 사망자수를 직관적으로 비교할 수 있다는 점에서 큰 장점이 있다.\n\ndeath_viz_bar_gg &lt;- death_viz |&gt; \n  ggplot() +\n    geom_col(aes(x = Date, y = 사망자수, fill = 사망원인), colour=\"white\") +\n    geom_vline(xintercept = as.Date(\"1855-03-15\"), linetype= 2) +\n    scale_fill_manual(values = c(\"firebrick\", \"orange\", \"#365181\")) + \n    # theme_ipsum_pub(base_family = noto_font, subtitle_family = noto_font) +\n    labs(title = \"크림전쟁 병사 사망원인\", \n         subtitle = \"데이터 시각화와 커뮤니케이션\", \n         caption = \"데이터 출처: 크림전쟁 사망자\",\n         x = \"월일\") + \n    scale_y_continuous(labels = scales::comma, limits = c(0, 1150)) +\n    theme(legend.position = \"top\", \n          text = element_text(family = noto_font, size = 18),\n          axis.ticks = element_blank(),\n          plot.margin = unit(rep(0.7, 4), \"cm\"),\n          plot.title = element_text(color = \"black\", family = noto_font, size = 35),\n          plot.caption = element_text(color = \"grey10\", family = noto_font, size = 17),\n          plot.subtitle = element_text(color = \"grey5\", size = 13),\n          legend.text = element_text(color = \"black\", size = 15)) +\n    geom_segment(x = as.Date(\"1854-03-01\"), y = 1100,\n                 xend = as.Date(\"1855-03-01\"), yend = 1100,\n                 color = \"gray70\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    geom_segment(x = as.Date(\"1855-04-01\"), y = 1100,\n                 xend = as.Date(\"1856-03-01\"), yend = 1100,\n                 color = \"gray15\",\n                 arrow = arrow(length = unit(0.1, \"inches\"))) +\n    annotate(\"text\", x = as.Date(\"1854-09-01\"), y = 1140, label = \"조치이전\",\n             size = 8.5, color = \"gray30\", family = noto_font) +\n    annotate(\"text\", x = as.Date(\"1855-09-01\"), y = 1140, label = \"조치이후\",\n             size = 8.5, color = \"gray15\", family = noto_font) \n\ndeath_viz_bar_gg\n\nragg::agg_jpeg(\"images/death_viz_bar_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\ndeath_viz_bar_gg\ndev.off()",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#표-문법",
    "href": "cs_nightingale.html#표-문법",
    "title": "7  나이팅게일",
    "section": "7.8 표 문법",
    "text": "7.8 표 문법\n데이터 문법, 그래프 문법에 이어 최근 “표 문법”이 새롭게 자리를 잡아가고 있다. 표 문법에 맞춰 나이팅게일 크림전쟁 사망자수를 조치 이전과 조치 이후로 나눠 요약하면 확연한 차이를 파악할 수 있다.\ngt와 gtExtras 패키지를 활용하여 death_viz 데이터프레임을 사망 원인별 사망자 수를 “조치 이전”과 “조치 이후”로 구분하여 표를 두개 생성한다. 각 표은 날짜, 질병, 부상, 기타 범주로 사망자 수와 그 합계를 표시하며, 총 사망자수가 250명을 초과하는 행에 대한 강조 색상을 입히고 나서 두 표를 나란히 배치하여 조치 전후 효과를 시각적으로 비교한다.\n\nlibrary(gt)\nlibrary(gtExtras)\n\nbefore_tbl &lt;- death_viz |&gt; \n  dplyr::filter(체제 == \"조치이전\")\n\nafter_tbl &lt;- death_viz |&gt; \n  dplyr::filter(체제 == \"조치이후\")\n\nbefore_gt &lt;- before_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n    tab_spanner(label = \"조치 이전\", columns = c(질병, 부상, 기타)) |&gt; \n    data_color(\n      columns = c(질병, 부상, 기타, 합계),\n      rows = 합계 &gt; 250,      \n      method = \"numeric\",\n      palette = \"ggsci::red_material\")\n\nafter_gt &lt;- after_tbl |&gt; \n  pivot_wider(names_from = 사망원인, values_from = 사망자수) |&gt; \n  select(날짜 = Date, 질병, 부상, 기타) |&gt; \n  mutate(합계 = 질병 + 부상 + 기타) |&gt; \n  gt() |&gt; \n    gt_theme_538() |&gt; \n    cols_align(\"center\") |&gt; \n    fmt_integer( columns = 질병:합계) |&gt; \n  tab_spanner(label = \"조치 이후\", columns = c(질병, 부상, 기타)) |&gt; \n  data_color(\n    columns = c(질병, 부상, 기타, 합계),\n    rows = 합계 &gt; 250,      \n    method = \"numeric\",\n    palette = \"ggsci::red_material\")\n\ngtExtras::gt_two_column_layout(list(before_gt, after_gt))\n\n\n\n\n\n\n\n\n\n날짜\n조치 이전\n합계\n\n\n질병\n부상\n기타\n\n\n\n\n1854-04-01\n1\n0\n7\n8\n\n\n1854-05-01\n6\n0\n5\n11\n\n\n1854-06-01\n5\n0\n2\n7\n\n\n1854-07-01\n150\n0\n10\n160\n\n\n1854-08-01\n328\n0\n12\n341\n\n\n1854-09-01\n312\n32\n28\n372\n\n\n1854-10-01\n197\n52\n50\n299\n\n\n1854-11-01\n341\n116\n43\n499\n\n\n1854-12-01\n632\n42\n48\n721\n\n\n1855-01-01\n1,023\n31\n120\n1,174\n\n\n1855-02-01\n823\n16\n140\n979\n\n\n1855-03-01\n480\n13\n69\n562\n\n\n\n\n\n\n\n\n\n\n\n날짜\n조치 이후\n합계\n\n\n질병\n부상\n기타\n\n\n\n\n1855-04-01\n178\n18\n21\n217\n\n\n1855-05-01\n172\n17\n12\n201\n\n\n1855-06-01\n248\n64\n10\n322\n\n\n1855-07-01\n108\n38\n9\n154\n\n\n1855-08-01\n130\n44\n7\n181\n\n\n1855-09-01\n48\n69\n5\n122\n\n\n1855-10-01\n33\n14\n5\n51\n\n\n1855-11-01\n56\n10\n10\n77\n\n\n1855-12-01\n25\n5\n8\n38\n\n\n1856-01-01\n11\n0\n13\n25\n\n\n1856-02-01\n7\n0\n5\n12\n\n\n1856-03-01\n4\n0\n9\n13",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_nightingale.html#커뮤니케이션",
    "href": "cs_nightingale.html#커뮤니케이션",
    "title": "7  나이팅게일",
    "section": "7.9 커뮤니케이션",
    "text": "7.9 커뮤니케이션\n데이터를 기반으로 뭔가 유용한 것을 창출한 후에 이를 알리기 위해 커뮤니케이션 단계를 거치게 된다. 가장 흔히 사용하는 방식은 엑셀, 워드, 파워포인트와 같은 MS 오피스 제품을 활용하는 방식이다. 과거 SAS, SPSS, 미니탭 등 외산 통계 팩키지로 데이터를 분석하고 유용한 모형 등을 찾아낸 후에 이를 커뮤니케이션하기 위해 MS 오피스 제품을 통해 커뮤니케이션을 하기도 했다. 하지만, 각각은 별개의 시스템으로 분리되어 있어 일일이 사람손이 가는 번거러움이 많았다. 이를 해결하기 하는 방법은 하나의 도구 혹은 언어로 모든 작업을 처리하는 것이다. [^meghan]\n[^meghan] : Meghan Hall (June 15, 2021), “Extending R Markdown”, RStudio: R in Sports Analytics,\n우선 엑셀은 tidyverse 로 대체가 되고, 워드는 R 마크다운을 거쳐 쿼토(Quarto), 파워포인트도 R 마크다운(xaringan 등)에서 진화한 reveal.js 기반 쿼토 슬라이드가 빠르게 자리를 잡아가고 있다.\n\n\n\n오피스 기반 커뮤니케이션 현재 상태점검\n\n\n데이터 과학을 커뮤니케이션하는 방식은 다양한 방식이 존재하지만 직장상사 뿐만 아니라 집단지성을 넘어 AI를 적극 도입하여 데이터 분석 역량을 고도화하는데 동료 개발자 및 협업하시는 분들과 커뮤니케이션 뿐만 아니라 불특정 다수를 대상으로 한 인터넷에 공개와 공유를 통해 새로운 관계를 맺어가는 것도 그 중요성을 더해가고 있다.\n\n동료 개발자나 협업하시는 분: .qmd 파일\n직장상사\n\nPDF 파일: quarto, pandoc\n파워포인트 슬라이스덱: reveal.js 기반 quarto\n대쉬보드: flexdashboard\n\n일반 공개\n\n웹사이트: distill을 지나 quarto\n블로그: blogdown을 지나 quarto\n책: bookdown을 지나 quarto",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>나이팅게일</span>"
    ]
  },
  {
    "objectID": "cs_napolean.html",
    "href": "cs_napolean.html",
    "title": "8  나폴레옹 러시아 침공",
    "section": "",
    "text": "8.1 미나르 지도\n찰스 조셉 미나르(Charles Joseph Minard)가 제작한 1869년 지도는 나폴레옹의 1812년 러시아 원정을 시각적으로 표현한 것으로, 다양한 데이터 시각화의 원칙과 기술을 혁신적으로 보여주는 명작으로 평가받고 있다. 나폴레옹 군대가 러시아로 진군하고 다시 귀환하는 과정에서 병력이 얼마나 줄어들었는지를 선의 두께로 표시했을 뿐만 아니라, 지도에는 군대 이동 경로, 위치, 날짜, 온도 등 다양한 정보가 함께 표시되어, 단순한 지리적 정보를 넘어서 시간과 조건에 따른 변화까지 한 눈에 파악할 수 있다.\n미나르의 지도는 데이터 시각화의 중요성과 효과를 입증하는 고전으로 여겨진다. 복잡하고 다양한 데이터를 단순하면서도 명확하게 표현함으로써, 복잡한 사건과 현상을 이해하기 쉽게 제작했다. 나폴레옹 군대의 군사적 실패 뿐만 아니라, 병참, 전략, 날씨가 군사 작전에 미치는 영향을 진군과 후퇴를 색상을 달리하여 표현하는 등 시각화를 통해 잘 보여주고 있다.\n저자를 포함한 많은 분들이 프랑스 파리에서 출발하여 러시아 원정이 시작되었다고 잘못알고 있으나, 1812년 러시아 원정 시작지는 니멘 강(Neman River)을 건너는 지점으로 니멘 강은 당시 러시아와 프랑스 연합국인 프러시아(당시 동유럽에 위치) 사이에서 경계를 형성하고 있었다. 1812년 6월 24일, 나폴레옹의 대군이 이 강을 건너면서 러시아 침공을 시작됐다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>나폴레옹 러시아 침공</span>"
    ]
  },
  {
    "objectID": "cs_napolean.html#미나르-지도",
    "href": "cs_napolean.html#미나르-지도",
    "title": "8  나폴레옹 러시아 침공",
    "section": "",
    "text": "나폴레옹 러시아 원정 미나르 지도\n\n\n\n\n\n\n미나르 지도를 현재 지도와 매핑",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>나폴레옹 러시아 침공</span>"
    ]
  },
  {
    "objectID": "cs_napolean.html#데이터",
    "href": "cs_napolean.html#데이터",
    "title": "8  나폴레옹 러시아 침공",
    "section": "8.2 데이터",
    "text": "8.2 데이터\n\n8.2.1 생존병사 수\nHistData 패키지에 나폴레옹 러시아 원정 데이터셋이 포함되어 있다. 원데이터셋은 minard.txt 파일에서 바로 얻을 수 있다.\nHistData, tidyverse, gt, gtExtras 패키지를 활용하여 나폴레옹 러시아 원정 데이터를 표형태로 출력한다.\n먼저 필요한 패키지를 불러온 다음, 병사 이동 경로(Minard.troops), 도시 정보(Minard.cities), 기온 데이터(Minard.temp)를 적재하고, filter() 함수를 사용하여 진격과 후퇴 상황에 따라 데이터를 나눠 attack_tbl과 retreat_tbl에 저장한다.\ngt() 함수로 데이터프레임을 표로 변화시키는데, gt_theme_hangul() 함수는 한글 테마를 적용하고, fmt_integer()는 정수 형태로 천단위 넘는 숫자 가독성을 높이고, cols_align() 함수로 칼럼을 가운데 정렬하고, cols_label()로 칼럼 라벨을 영어에서 한글로 변환한다. tab_header() 함수로 진격과 후퇴 제목을 설정하여 attack_gt와 retreat_gt에 각각 저장한다. 마지막으로 gtExtras::gt_two_column_layout() 함수를 사용하여 두 표를 병합하여 마무리한다.\n\nlibrary(HistData)\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\n\ndata(Minard.troops) \ndata(Minard.cities)\ndata(Minard.temp)\n\nattack_tbl &lt;- Minard.troops |&gt; \n  dplyr::filter(direction == \"A\")\n\nretreat_tbl &lt;- Minard.troops |&gt; \n  dplyr::filter(direction == \"R\")\n\nattack_gt &lt;- attack_tbl |&gt; \n  gt() |&gt; \n    gt_theme_hangul() |&gt; \n    fmt_integer(columns = survivors) |&gt; \n    cols_align(\"center\") |&gt; \n    cols_label(\n      long = \"경도\",\n      lat = \"위도\",\n      survivors =  \"생존병사수\",\n      direction = \"진격방향\"\n    ) |&gt; \n    tab_header(title = \"진격경로\")\n\nretreat_gt &lt;- retreat_tbl |&gt; \n  gt() |&gt; \n    gt_theme_hangul() |&gt; \n    fmt_integer(columns = survivors) |&gt; \n    cols_align(\"center\") |&gt; \n    cols_label(\n      long = \"경도\",\n      lat = \"위도\",\n      survivors =  \"생존병사수\",\n      direction = \"진격방향\"\n    )  |&gt; \n    tab_header(title = \"후퇴\")\n\nattack_retreat_gt &lt;- gtExtras::gt_two_column_layout(list(attack_gt, retreat_gt))\n\n# gtsave_extra(attack_retreat_gt, \"images/attack_retreat_gt.png\")\n\n\n\n\n8.2.2 후퇴 기온\nMinard.temp 데이터셋을 사용하여 후퇴 시점의 온도 정보를 표로 제작한다. 먼저 mutate() 함수를 통해 date 칼럼 값을 문자열로 변환하고, “1812” 연도를 추가해 날짜 정보를 완성한다. mdy() 함수를 사용해 날짜 문자열을 Date 자료형으로 변환하는데, quiet = TRUE 옵션을 설정해 경고 메시지를 표시하지 않게 한다.\n이후에는 gt() 함수를 사용해 데이터 프레임을 테이블로 변환하게 된다. gt_theme_hangul()을 통해 테이블에 한국어 테마를 적용하고, cols_align(\"center\")로 모든 칼럼을 가운데 정렬한다. cols_label() 함수를 통해 각 칼럼에 적절한 한국어 레이블을 지정한다. 만약\ngt() 함수로 데이터프레임을 표로 변화시키는데, gt_theme_hangul() 함수는 한글 테마를 적용하고, fmt_integer()는 정수 형태로 천단위 넘는 숫자 가독성을 높이고, cols_align() 함수로 칼럼을 가운데 정렬하고, cols_label()로 칼럼 라벨을 영어에서 한글로 변환한다. 관측날짜 칼럼에 누락된 값이 있으면, fmt_missing(missing_text = \"-\")를 사용하여 누락된 값을 “-”로 표시한다.\nfmt_date() 함수를 사용하여 date 칼럼의 날짜 형식을 date_style = \"yMMMd\"와 locale = \"ko\"를 설정하여 연도, 월, 일을 대한민국 기준에 맞춘다.\n\nMinard.temp |&gt; \n  mutate(date = as.character(date), # date를 문자열로 변환\n         date = str_glue(\"{date}/1812\")) |&gt; \n  mutate(date = mdy(date, quiet = TRUE)) |&gt;  # quiet = TRUE로 경고 메시지 방지\n\n  gt() |&gt; \n    gt_theme_hangul() |&gt; \n    # fmt_integer(columns = survivors) |&gt; \n    cols_align(\"center\") |&gt; \n    cols_label(  \n      long = \"위도\",\n      temp = \"기온\",\n      days = \"후퇴날짜\",\n      date = \"관측날짜\") |&gt; \n    tab_header(title = \"후퇴 온도\") |&gt; \n    gt::fmt_missing(missing_text = \"-\") |&gt; \n    fmt_date(\n      columns = date,\n      date_style  = \"yMMMd\",\n      locale = \"ko\"\n    )\n\n\n\n\n\n\n\n후퇴 온도\n\n\n위도\n기온\n후퇴날짜\n관측날짜\n\n\n\n\n37.6\n0\n6\n1812년 10월 18일\n\n\n36.0\n0\n6\n1812년 10월 24일\n\n\n33.2\n-9\n16\n1812년 11월 9일\n\n\n32.0\n-21\n5\n1812년 11월 14일\n\n\n29.2\n-11\n10\n-\n\n\n28.5\n-20\n4\n1812년 11월 28일\n\n\n27.2\n-24\n3\n1812년 12월 1일\n\n\n26.7\n-30\n5\n1812년 12월 6일\n\n\n25.3\n-26\n1\n1812년 12월 7일",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>나폴레옹 러시아 침공</span>"
    ]
  },
  {
    "objectID": "cs_napolean.html#시각화",
    "href": "cs_napolean.html#시각화",
    "title": "8  나폴레옹 러시아 침공",
    "section": "8.3 시각화",
    "text": "8.3 시각화\n나폴레옹의 러시아 침공을 앞선 데이터셋을 바탕으로 시각화한다. HistData 패키지에서 제공하는 Minard 데이터셋을 이용하여 병력의 이동 경로와 생존 병사 수를 그리고(plot_troops), 주요 도시의 위치를 표시한다(plot_cities). 두 그래프를 합쳐 하나의 그래프(plot_minard)로 만들고, 그 위에 온도 변화(plot_temp)까지 표시한다. 마지막으로 grid.arrange 함수를 사용해 두 그래프를 하나로 합치고, 이를 이미지 파일로 저장한다.\n병력 이동, 생존자 수, 도시 위치, 그리고 온도 변화, 진격과 후퇴 색상을 달리하여 나폴레옹 러시아 침공에 대한 전반적인 사항을 한눈에 볼 수 있는 시각화 그래프를 제작했다.\n\nlibrary(HistData)\nlibrary(ggrepel)\nrequire(scales)\nrequire(gridExtra)\n\ndata(Minard.troops)\ndata(Minard.cities)\ndata(Minard.temp)\n\nlevels(Minard.cities$city) &lt;- c(\"Bobr\", \"Chjat\", \"Dorogobouge\", \"Gloubokoe\", \"Kowno\", \"Malo-Jarosewii\", \n                                \"Minsk\", \"Mohilow\", \"Moiodexno\", \"Mojaisk\", \"모스코바\", \"Orscha\", \n                                \"Polotzk\", \"Smolensk\", \"Smorgoni\", \"Studienska\", \"Tarantino\", \n                                \"Wilna\", \"Witebsk\", \"Wixma\")\n\n# 병력 이동 경로와 도시 이름을 레이어에 올린다.\nplot_troops &lt;- ggplot(Minard.troops, aes(long, lat)) +\n    geom_path(aes(size = survivors, colour = direction, group = group),\n               lineend = \"round\", linejoin = \"round\")\nplot_cities &lt;- geom_text(aes(label = city), size = 4, data = Minard.cities)\n \n# 눈금 정보, 라벨을 추가\n# 온도를 맞추도록 경도에 대한 x축 명시적 설정.\n\nbreaks &lt;- c(1, 2, 3) * 10^5 \n\nplot_minard &lt;- plot_troops + plot_cities +\n    scale_size(\"생존병사수\", range = c(1, 10), \n                breaks = breaks, labels = scales::comma(breaks)) +\n  scale_color_manual(\"진격방향\", \n                     values = c(\"grey50\", \"red\"), \n                     labels=c(\"진격\", \"후퇴\")) +\n  coord_cartesian(xlim = c(24, 38)) +\n  labs(x = NULL,\n       y = \"경도\",\n       title = \"나폴레옹 러시아 침공\",\n       subtitle = \"1812년 6월 24일 ~ 1813년 1월 5일\") +\n  theme_korean() +\n  theme(legend.position=c(.8, .2), legend.box=\"horizontal\")\n \nplot_temp &lt;- Minard.temp %&gt;% \n  mutate(date = case_when(str_detect(date, \"Oct\") ~ str_replace(date, \"Oct\", \"10월\"),\n                          str_detect(date, \"Nov\") ~ str_replace(date, \"Nov\", \"11월\"),\n                          str_detect(date, \"Dec\") ~ str_replace(date, \"Dec\", \"12월\"),\n                           TRUE ~ \"미상\")) %&gt;% \n  mutate(date = glue::glue(\"{date}일\")) %&gt;% \n  mutate(date = ifelse(date == \"미상일\", \"미상\", date)) %&gt;% \n  ggplot(aes(long, temp)) +\n    geom_path(color=\"grey\", size=1.5) +\n    geom_point(size=2) +\n    geom_text_repel(aes(label=glue::glue(\"{date}\")) ) +\n    xlab(\"위도\") + ylab(\"기온\") +\n    coord_cartesian(xlim = c(24, 38)) + \n    theme_korean()\n    \n# 그래프 결합\nminard_g &lt;- grid.arrange(plot_minard, plot_temp, nrow=2, heights=c(3,1))\n\nggsave( glue::glue(\"images/나폴레옹_러시아.png\") , \n        minard_g,\n        device = ragg::agg_png, \n        width = 297, height = 210, units = \"mm\", res = 600)",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>나폴레옹 러시아 침공</span>"
    ]
  },
  {
    "objectID": "u_boats.html",
    "href": "u_boats.html",
    "title": "3  유보드",
    "section": "",
    "text": "3.1 유보트 데이터\n총 1,153대 유보트가 활동했으며 전쟁이 끝난 후에 유보트 운명을 표로 정리하면 다음과 같다.\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\n\nuboat_raw &lt;- read_csv(\"data/uboat-data.csv\")\n\nfate_type_eng &lt;- c(\"Sunk\", \"Scuttled\", \"Surrendered\", \"Decommissioned\", \"Missing\", \n\"Given\", \"Damaged\", \"Captured\", \"Grounded\", \"Destroyed\")\n\nfate_type_kor &lt;- c(\"침몰\", \"자침\", \"항복\", \"해체\", \"실종\", \"기증\", \"손상\", \"포획\", \"좌초\", \"파괴\")\n\nfate_translation &lt;- tibble(fate_type = fate_type_eng,\n                           fate_type_kor = fate_type_kor)\n\nuboat_raw |&gt; \n  count(fate_type, sort = TRUE, name = \"대수\") |&gt; \n  mutate(fate_kor = c(\"침몰\", \"자침\", \"항복\", \"해체\", \"실종\", \"기증\", \"손상\", \"포획\", \"좌초\", \"파괴\")) |&gt; \n  mutate(비율 = 대수 / sum(대수)) |&gt;  \n  relocate(fate_kor, .after = fate_type) |&gt; \n  gt() |&gt; \n    gt_theme_hangul() |&gt;   \n    cols_label(\n        fate_type = md(\"**fate type**\"),\n        fate_kor = md(\"**유보트 운명**\"),\n    ) |&gt; \n    grand_summary_rows(\n      columns = 대수,\n      fns =  list(label = \"합계\", id='totals', fn = \"sum\"),\n      fmt = ~ fmt_integer(.),\n      side = \"bottom\"\n    ) |&gt; \n    grand_summary_rows(\n      columns = 비율,\n      fns =  list(label = \"합계\", fn = \"sum\"),\n      fmt = ~ fmt_percent(., decimals = 0),\n      side = \"bottom\"\n    ) |&gt; \n    fmt_percent(columns = 비율, decimals = 1) |&gt; \n    cols_align(\"center\") |&gt; \n    tab_header(\n      title = \"유보트 유형별 운명\",\n      subtitle = \"자료출처: uboat.net\"\n    )\n\n\n\n\n\n\n\n\n유보트 유형별 운명\n\n\n자료출처: uboat.net\n\n\n\nfate type\n유보트 운명\n대수\n비율\n\n\n\n\n\nSunk\n침몰\n640\n55.5%\n\n\n\nScuttled\n자침\n218\n18.9%\n\n\n\nSurrendered\n항복\n154\n13.4%\n\n\n\nDecommissioned\n해체\n55\n4.8%\n\n\n\nMissing\n실종\n46\n4.0%\n\n\n\nGiven\n기증\n16\n1.4%\n\n\n\nDamaged\n손상\n12\n1.0%\n\n\n\nCaptured\n포획\n5\n0.4%\n\n\n\nGrounded\n좌초\n4\n0.3%\n\n\n\nDestroyed\n파괴\n3\n0.3%\n\n\n합계\n—\n—\n1,153\n100%",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>유보드</span>"
    ]
  },
  {
    "objectID": "u_boats.html#유보트-데이터",
    "href": "u_boats.html#유보트-데이터",
    "title": "3  유보드",
    "section": "",
    "text": "3.1.1 유보트 성과\n유보트가 침몰시킨 선박수(ships_sunk) 정보가 담겨있어 이를 바탕으로 활동성과를 분석해보자.\n\nuboat_tbl &lt;- uboat_raw |&gt; \n  # filter( fate_type == \"Sunk\") |&gt; \n  mutate(fate_date = lubridate::mdy(fate)) \n\nuboat_tbl |&gt; \n  group_by(fate_type) |&gt; \n  summarise(침몰선박수 = sum(ships_sunk),\n            유보트수 = n()) |&gt; \n  mutate(비율 = 침몰선박수 / sum(침몰선박수)) |&gt; \n  arrange(desc(침몰선박수)) |&gt; \n  left_join(fate_translation) |&gt; \n  relocate(fate_type_kor, .after = fate_type) |&gt; \n  relocate(유보트수, .after = fate_type_kor) |&gt; \n  gt::gt() |&gt; \n  gt_theme_hangul() |&gt; \n  cols_align(\"center\") |&gt; \n  gt::tab_header(\n    title = \"유보트 운명유형별 침몰선박수\", \n    subtitle = \"자료출처: uboat.net\"\n  ) |&gt; \n    cols_label(\n        fate_type = md(\"**fate type**\"),\n        fate_type_kor = md(\"**유보트 운명**\"),\n    ) |&gt; \n    grand_summary_rows(\n      columns = 침몰선박수,\n      fns =  list(label = \"합계\", id='totals', fn = \"sum\"),\n      fmt = ~ fmt_integer(.),\n      side = \"bottom\"\n    ) |&gt; \n    grand_summary_rows(\n      columns = 비율,\n      fns =  list(label = \"합계\", fn = \"sum\"),\n      fmt = ~ fmt_percent(., decimals = 0),\n      side = \"bottom\"\n    ) |&gt; \n    fmt_percent(columns = 비율, decimals = 1) |&gt; \n    fmt_integer(침몰선박수) |&gt; \n    tab_spanner(label = \"침몰선박\", columns = c(침몰선박수, 비율))\n\n\n\n\n\n\n\n\n유보트 운명유형별 침몰선박수\n\n\n자료출처: uboat.net\n\n\n\nfate type\n유보트 운명\n유보트수\n침몰선박\n\n\n침몰선박수\n비율\n\n\n\n\n\nSunk\n침몰\n640\n1,809\n65.8%\n\n\n\nDecommissioned\n해체\n55\n373\n13.6%\n\n\n\nScuttled\n자침\n218\n265\n9.6%\n\n\n\nSurrendered\n항복\n154\n131\n4.8%\n\n\n\nMissing\n실종\n46\n101\n3.7%\n\n\n\nGiven\n기증\n16\n41\n1.5%\n\n\n\nDamaged\n손상\n12\n16\n0.6%\n\n\n\nCaptured\n포획\n5\n12\n0.4%\n\n\n\nDestroyed\n파괴\n3\n2\n0.1%\n\n\n\nGrounded\n좌초\n4\n1\n0.0%\n\n\n합계\n—\n—\n—\n2,751\n100%\n\n\n\n\n\n\n\n\n\n# extrafont::font_import(pattern = \"NanumSquare\", prompt = FALSE)\n# extrafont::loadfonts()\n\nuboat_trends_gg &lt;- uboat_tbl |&gt; \n  mutate(fate_yearmon = floor_date(fate_date, \"month\")) |&gt; \n  group_by(fate_yearmon) |&gt; \n  summarise(침몰선박수 = sum(ships_sunk)) |&gt; \n  arrange(desc(침몰선박수)) |&gt; \n  ggplot(aes(x = fate_yearmon, y = 침몰선박수)) +\n    geom_line() +\n    theme_korean() +\n    labs(x =\"\",\n         title = \"월별 유보트 침몰선박수\")\n\nuboat_trends_gg\n\nragg::agg_jpeg(\"images/uboat_trends_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\nuboat_trends_gg\ndev.off()\n\n\n\nlibrary(giscoR)\nlibrary(sf)\n\nworld &lt;- gisco_get_countries()\n\nsunk_sf &lt;- uboat_tbl |&gt; \n  filter(fate_type  == \"Sunk\")  |&gt; \n  mutate(sunk_year = year(fate_date)) |&gt; \n  select(name, fat_lon, fate_lat, sunk_year) |&gt; \n  mutate(across(fat_lon:fate_lat, as.numeric)) |&gt; \n  mutate(sunk_year = as.factor(sunk_year)) |&gt; \n  drop_na() |&gt; \n  st_as_sf(coords = c(\"fat_lon\", \"fate_lat\"), crs = sf::st_crs(world))\n\nsunk_sf_gg &lt;- sunk_sf |&gt; \n  mutate(sunk_year = case_when(sunk_year %in% c(1939, 1940) ~ \"1939-40\",\n                   TRUE ~ sunk_year)) |&gt; \n  ggplot() +\n    geom_sf(size = 0.5, aes(color = sunk_year)) +\n    geom_sf(data = world) +\n    facet_wrap( ~ sunk_year ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    labs(title = \"연도별 침몰 유보트 좌표\")\n\nsunk_sf_gg\n\nragg::agg_jpeg(\"images/sunk_sf_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\nsunk_sf_gg\ndev.off()",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>유보드</span>"
    ]
  },
  {
    "objectID": "u_boats.html#목표선박-데이터",
    "href": "u_boats.html#목표선박-데이터",
    "title": "3  유보드",
    "section": "3.2 목표선박 데이터",
    "text": "3.2 목표선박 데이터\nGitHub 저장소에 침몰선박에 대한 정보도 확인해보자.\n\ndownload.file(url = \"https://raw.githubusercontent.com/kadenhendron/uboat-data/master/data/uboat-target-data.csv\", \n              destfile = \"data/uboat-target-data.csv\", model = \"w\")\n\n국적별 총 침몰선박수를 표로 작성한다.\n\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(countrycode)\n\ntarget_raw &lt;- read_csv(\"data/uboat-target-data.csv\")\n\ntarget_tbl &lt;- target_raw |&gt; \n  mutate(attack_date = lubridate::mdy(attack_date)) |&gt; \n  mutate(yearmon = floor_date(attack_date, \"month\")) \n\nships_nationality &lt;- target_tbl |&gt; \n  mutate(nationality = fct_lump(nationality, n = 9, other_level = \"기타국가\")) |&gt; \n  group_by(nationality) |&gt; \n  summarise(선박수 = n()) |&gt; \n  arrange(desc(선박수))\n\ntop10_iso &lt;- tribble(~\"nationality\", ~\"iso3c\", ~\"country_name\", ~\"country_name_kr\",\n\"British\", \"GBR\",  \"United Kingdom\", \"영국\",\n\"American\", \"USA\",  \"United States\", \"미국\",\n\"기타국가\", \"\", \"ETC\", \"기타\",\n\"Norwegian\", \"NOR\",  \"Norway\", \"노르웨이\",\n\"Dutch\", \"NLD\",  \"Denmark\", \"덴마크\", # 주의: Dutch는 네덜란드 사람을 나타내며, Denmark는 덴마크를 의미합니다.\n\"Greek\", \"GRC\",  \"Greece\", \"그리스\",\n\"Soviet\", \"SUN\", \"Russia\", \"러시아\", # 주의: Soviet는 소비에트 연방을 나타냅니다. 현대의 러시아와는 다름니다.\n\"Swedish\", \"SWE\",  \"Sweden\", \"스웨덴\",\n\"Panamanian\", \"PAN\",  \"Panama\", \"파나마\",\n\"Canadian\", \"CAN\",  \"Canada\", \"캐나다\")\n\ntarget_uboat_gt &lt;- target_tbl |&gt; \n  mutate(nationality = fct_lump(nationality, n = 9, other_level = \"기타국가\")) |&gt; \n  group_by(nationality, yearmon) |&gt; \n  summarise(선박수 = n()) |&gt; \n  group_by(nationality) |&gt; \n  summarise(ships_data = list(선박수)) |&gt;  \n  left_join(ships_nationality) |&gt; \n  arrange(desc(선박수)) |&gt; \n  left_join(top10_iso) |&gt; \n  mutate(iso_2 = countrycode(country_name, origin = \"country.name\", \"iso2c\")) |&gt; \n  relocate(iso_2, .before = nationality) |&gt; \n  select(iso_2, 국가명 = country_name_kr, 선박수, ships_data) |&gt; \n  mutate(비율 = 선박수 / sum(선박수)) |&gt; \n  relocate(비율, .after = 선박수) |&gt; \n  gt() |&gt; \n    gtExtras::gt_plt_sparkline(ships_data) |&gt; \n    fmt_flag(columns = iso_2) |&gt; \n    cols_label(\n      iso_2 = \"\",\n      선박수 = \"총 침몰선박수\",\n      ships_data = \"월별 선박침몰\"\n    )  |&gt; \n    gt_theme_hangul() |&gt; \n    gt::tab_header(\n      title = \"국가별 피해 침몰선박수\", \n      subtitle = \"자료출처: uboat.net\"\n    ) |&gt; \n      grand_summary_rows(\n        columns = 선박수,\n        fns =  list(label = \"합계\", id='totals', fn = \"sum\"),\n        fmt = ~ fmt_integer(.),\n        side = \"bottom\"\n      ) |&gt; \n      grand_summary_rows(\n        columns = 비율,\n        fns =  list(label = \"합계\", fn = \"sum\"),\n        fmt = ~ fmt_percent(., decimals = 0),\n        side = \"bottom\"\n      ) |&gt; \n      fmt_percent(columns = 비율, decimals = 1) |&gt; \n      fmt_integer(columns = 선박수) |&gt; \n      cols_align(\"center\") |&gt; \n      gt::fmt_missing(missing_text = \"-\")\n\ngtsave(target_uboat_gt, \"images/target_uboat_gt.png\")",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>유보드</span>"
    ]
  },
  {
    "objectID": "u_boats.html#유보트-사령관",
    "href": "u_boats.html#유보트-사령관",
    "title": "3  유보드",
    "section": "3.3 유보트 사령관",
    "text": "3.3 유보트 사령관\nRiding tables with {gt} and {gtExtras}\n\ncommander_uboat &lt;- target_tbl |&gt; \n  count(commander, name) |&gt; \n  group_by(commander) |&gt; \n  summarise(잠수함 = str_c(name, collapse = \",\")) |&gt; \n  mutate(잠수함 = str_split(잠수함, pattern = \",\")) |&gt; \n  unnest(잠수함) |&gt; \n  group_by(commander) |&gt; \n  summarise(잠수함 = str_c(잠수함, collapse = \", \"))\n\ntarget_commander_gt &lt;- target_tbl |&gt; \n  group_by(commander) |&gt; \n  summarise(선박수 = n(),\n            사망자수 = sum(dead, na.rm = TRUE),\n            톤수 = sum(tonnage, na.rm = TRUE)) |&gt; \n  arrange(desc(선박수)) |&gt; \n  mutate(선박수_그래프 = 선박수) |&gt; \n  left_join(commander_uboat) |&gt; \n  slice_max(order_by = 선박수, n = 10) |&gt; \n  # 시각화 \n  gt::gt() |&gt; \n    cols_label(\n      commander = \"유보트 선장\",\n      선박수 = \"선박수\",\n      잠수함 = \"탑승 잠수함\",\n      선박수_그래프 = \"그래프\"\n    )  |&gt; \n    gt_theme_hangul() |&gt; \n    gt::tab_header(\n      title = \"상위 10 유보트 선장\", \n      subtitle = \"자료출처: uboat.net\"\n    ) |&gt; \n    gtExtras::gt_merge_stack(col1 = commander, col2 = 잠수함) |&gt; \n    cols_width(\n      commander ~ px(130),\n      선박수_그래프 ~ px(100)\n    ) |&gt; \n    fmt_integer(columns = is.numeric) |&gt; \n    gt_plt_bar_pct(column = 선박수_그래프, scaled = FALSE, fill = \"blue\", background = \"lightblue\") |&gt; \n    gt::tab_spanner(label = \"총 침몰선박\", columns = c(선박수, 선박수_그래프)) |&gt; \n    cols_align(\"center\")\n\ngtsave(target_commander_gt, \"images/target_commander_gt.png\")",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>유보드</span>"
    ]
  },
  {
    "objectID": "u_boats.html#유보트와-선박",
    "href": "u_boats.html#유보트와-선박",
    "title": "3  유보드",
    "section": "3.4 유보트와 선박",
    "text": "3.4 유보트와 선박\n침몰된 선박과 유보트 손실을 월별 시각화를 통해서 1943년을 기점으로 유보트에 대한 연합국의 대응이 효과를 발휘하여 선박손실을 급격히 줄어든 반면 유보트 손실이 급격히 올라간 것을 확인할 수 있다.\n\nuboat_yearmon &lt;- uboat_tbl |&gt; \n  mutate(년월 = floor_date(fate_date, \"month\")) |&gt; \n  group_by(년월) |&gt; \n  summarise(유보트수 = n())\n\ntarget_yearmon &lt;- target_tbl |&gt; \n  mutate(년월 = floor_date(attack_date, \"month\")) |&gt; \n  group_by(년월) |&gt; \n  summarise(침몰수 = n()) \n\nuboat_yearmon_gg &lt;- uboat_yearmon |&gt; \n  left_join(target_yearmon) |&gt; \n  pivot_longer(cols = 유보트수:침몰수, names_to = \"구분\", values_to = \"선박수\") |&gt; \n  ggplot(aes(x = 년월, y = 선박수, color = 구분)) +\n    geom_line() +\n    labs(title = \"침몰 유보트와 침몰 선박수 추세\",\n         x = \"\",\n         y = \"침몰선박수\",\n         caption = \"자료출처: uboats.net\") +\n    theme_minimal() +\n    scale_color_manual(values = c(\"red\", \"blue\")) +\n    theme(legend.position = \"top\")\n\nragg::agg_jpeg(\"images/uboat_yearmon_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\nuboat_yearmon_gg\ndev.off()\n\n\n\n\n\n\nHendron, Kaden. 2016. “Germany’s U-Boats & Data Visualization: Can data visualization offer answers to our questions about history?” 2016년 4월 20일. https://medium.com/@kadenhendron/germany-s-u-boats-data-visualization-6e018c6c174.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>유보드</span>"
    ]
  },
  {
    "objectID": "cs_prussia.html",
    "href": "cs_prussia.html",
    "title": "10  프로이센 기병",
    "section": "",
    "text": "10.1 교통사고\n포아송은 미래에 발생할 경우의 수를 예측하기 위해서 포아송 분포를 창안했다. 좀더 구체적으로 고정된 시간 범위에 발생할 사건을 예측하기 위해서다.\n한가지 사례로 한국 R 사용자회 페이스북 그룹에 매주 페이스북 게시글을 올리는데 좋아요를 누르는 평균 회원수가 10명이다. 새로운 페이스북 게시글을 올렸는데 좋아요를 누른 회원이 15명이 될 확률은 얼마나 될까?\n이와 같이 다음주(미래) 좋아요를 누른(사건) 회원수가 15명(5, 10, 20, …)이 될 확률을 알고 싶은 것이다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>프로이센 기병</span>"
    ]
  },
  {
    "objectID": "cs_prussia.html#교통사고",
    "href": "cs_prussia.html#교통사고",
    "title": "10  프로이센 기병",
    "section": "",
    "text": "10.1.1 포아송 분포 수렴\n다음 조건을 만족할 때 이항분포가 포아송 분포로 수렴되어 근사할 수 있다.\n\n시행 횟수 \\(n\\) 이 매우 크다.\n성공 확률 \\(p\\) 가 매우 작다.\n따라서, \\(\\lambda = n \\times p\\) 가 일정하다.\n\n이항분포 \\(Bin(n, p)\\)는 포아송 분포 \\(Poi(\\lambda)\\)에 근사한다.\n전체 제품 중에서 고장확률이 매우 작은 전자제품을 사례로 들어보자. 예를 들어, 어떤 공장에서 10,000개의 제품을 제조했을 때, 각 제품이 고장날 확률이 0.0001이라고 가정하면 이항분포로 전체 제품 중 1개 고장확률을 계산할 수 있지만, 제품 수가 매우 크고 고장 확률이 매우 작기 때문에 \\(\\lambda = np = 10,000 \\times 0.0001 = 1\\)를 갖는 포아송 분포를 사용하여 근사할 수 있다.\n\n\n10.1.2 월간 교통사고\n한 도시의 주요 교차로에서, 지난 1년 동안의 데이터를 기반으로 하루 평균 3건의 교통 사고가 발생했다고 가정하자. 이 정보를 바탕으로 특정 날에 교통 사고가 발생할 횟수의 확률 분포를 예측해보자.\n포아송 분포의 평균은 \\(\\lambda\\)이며, 이 경우에는 하루 평균 교통 사고 횟수인 3으로 설정할 수 있다.\n이제 포아송 분포의 확률 질량 함수를 사용하여, 특정 날에 교통 사고가 k번 발생할 확률을 계산할 수 있다.\n\\[\nP(X=k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\\]\n여기서, \\(e\\)는 자연상수입니다.\n예를 들어, 특정 날에 교통 사고가 정확히 2번 발생할 확률을 계산하려면:\n\\[P(X=2) = \\frac{3^2 e^{-3}}{2!} = 0.224\\]\n한걸음 더 들어가 실세 교통사고분석시스템(TAAS) 웹사이트에서 2022년 월별 교통사고 데이터를 얻을 수 있다.\n\nlibrary(tidyverse)\nlibrary(readxl)\n\naccident_raw &lt;- read_excel(\"data/2022년_월별_교통사고.xlsx\", sheet = \"2022년도 월별 교통사고\", skip = 2)\n\naccident_tbl &lt;- accident_raw |&gt; \n  janitor::clean_names(ascii = FALSE) |&gt; \n  select(월, 사고건수 = 사고건수_건) \n\naccident_tbl\n#&gt; # A tibble: 12 × 2\n#&gt;    월    사고건수\n#&gt;    &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1 01월     15894\n#&gt;  2 02월     12362\n#&gt;  3 03월     13620\n#&gt;  4 04월     16472\n#&gt;  5 05월     18065\n#&gt;  6 06월     16481\n#&gt;  7 07월     17115\n#&gt;  8 08월     16496\n#&gt;  9 09월     17216\n#&gt; 10 10월     18508\n#&gt; 11 11월     17578\n#&gt; 12 12월     17029\n\n월별 평균 사고건수를 mean() 함수를 사용해서 계산할 수 있다. 교통량은 상당히 크고, 교통사고 확률은 매우 낮기 때문에 포아송 분포로 근사를 하는 것이 가능하다. 교통사고 건수가 많기 때문에 단위를 천대 기준으로 조정하여 포아송 분포 모수 \\(\\lambda\\)를 계산한다.\n\naccident_mean &lt;- mean(accident_tbl$사고건수) / 1000\n\naccident_mean\n#&gt; [1] 16.403\n\n월별로 2만대 이상 교통사고가 발생될 경우 교통사고 환자수가 급증하여 병원에 큰 부하가 걸려 사회적 문제가 된다는 가정하에 월별로 2만대 이상 교통사고가 발생할 확률을 계산해보자.\n\\(P(X \\geq 20)\\) 확률값은 전체 경우의 수에서 0 ~ 1.9 만대 사고건수가 발생할 확률을 빼주면 계산이 가능하고 다음과 같이 수식으로 표현할 수 있다.\n\\[\nP(X \\geq 20) = 1 - (P(X=0) + P(X=1) + \\ldots + P(X=19))\n\\]\n이를 R 코드로 작성하면 다음과 같이 함수형 프로그래밍 purrr 패키지 map_dbl() 함수와 포아송 함수에 \\(\\lambda = 16.403\\)를 넣어 계산이 가능하거나 내장 함수 ppois()로 직접 동일한 계산작업을 수행할 수 있다.\n\nlibrary(tidyverse)\n\naccident_prob &lt;- 1 - sum(map_dbl(0:19, ~ (accident_mean^.x * exp(-accident_mean)) / factorial(.x)))\n# 1 - ppois(19, lambda = 16.403)\n\naccident_prob\n#&gt; [1] 0.2169535",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>프로이센 기병</span>"
    ]
  },
  {
    "objectID": "cs_prussia.html#프로이센-기병-사망자",
    "href": "cs_prussia.html#프로이센-기병-사망자",
    "title": "10  프로이센 기병",
    "section": "10.2 프로이센 기병 사망자",
    "text": "10.2 프로이센 기병 사망자\n프로이센 기병대에서 말 발길질로 사망한 병사의 수를 1875년부터 1894년까지, 14개의 기병 군단을 대상으로 수집한 데이터(Prussian Horse-Kick Data)가 포아송 분포에 잘 적합되는 것으로 유명하다.\n\n원본 데이터를 디지털로 복원한 후에 고정된 기간 말 발차기 사망자수를 빈도통계를 통해 표로 정리할 수 있다. 총 관측 횟수는 \\(14 \\times 20 = 280\\) (즉, 1875년부터 1894년까지 20년간 프로이센 군단 14개를 관측), 총 사망자 병사수가 196명으로부터 평균 사망병사수를 \\(\\lambda = \\frac{196}{280} = 0.7\\) 으로 계산할 수 있다. 다음으로 포아송분포에 적합시켜서 분포로부터 말 발차기 사망자수 빈도수를 계산한다.\n\nlibrary(rvest)\nlibrary(gt)\nlibrary(gtExtras)\n\nkick_raw &lt;- read_html(x = 'https://www.randomservices.org/random/data/HorseKicks.html') |&gt; \n  html_node(\"table\") |&gt; \n  html_table()\n\n# kick_raw |&gt; \n#   write_csv(\"data/horse_kick.csv\")\n\nkick_tbl &lt;- kick_raw |&gt; \n  pivot_longer(-Year, names_to = \"군단\", values_to = \"병사수\") |&gt; \n  count(사망횟수 = 병사수, name = \"빈도수\") |&gt; \n  mutate(사망자수 = 사망횟수 * 빈도수)  |&gt; \n  mutate(포아송적합 = map_dbl(사망횟수, dpois, lambda = 196/280) * 280) |&gt; \n  mutate(포아송적합 = round(포아송적합, digits = 0)) |&gt; \n  janitor::adorn_totals(c(\"row\"), name = \"합계\")\n\nkick_tbl |&gt; \n  gt() |&gt; \n  gt_theme_538() |&gt; \n  cols_align(\"center\") |&gt; \n  gt::tab_spanner(label = \"데이터\", \n                  columns = c(사망횟수, 빈도수))\n\n\n\n\n\n\n\n데이터\n사망자수\n포아송적합\n\n\n사망횟수\n빈도수\n\n\n\n\n0\n144\n0\n139\n\n\n1\n91\n91\n97\n\n\n2\n32\n64\n34\n\n\n3\n11\n33\n8\n\n\n4\n2\n8\n1\n\n\n합계\n280\n196\n279\n\n\n\n\n\n\n\n시각적으로 실제 관측한 빈도수와 포아송 분포로부터 추정한 값을 함께 겹칠 경우 일부 차이가 있긴 하지만 대체로 포아송 분포에 잘 적합됨을 확인할 수 있다.\n\nkick_tbl |&gt; \n  dplyr::filter(사망횟수 != \"합계\") |&gt; \n  ggplot() +\n    geom_segment(aes(x = 사망횟수, xend = 사망횟수, y = 0, yend=빈도수),\n                 linewidth= 2) +\n    geom_point(aes(사망횟수, 포아송적합), size=3, color=\"red\") +\n    labs(x = \"말 발차기로 사망한 병사 수\",\n         y = \"빈도수\",\n         title = \"말 발길질로 인한 프로이센 병사 사망\",\n         subtitle = \"실제 관측 데이터와 포아송분포 적합 기대값\")",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>프로이센 기병</span>"
    ]
  },
  {
    "objectID": "cs_prussia.html#v2-rocket",
    "href": "cs_prussia.html#v2-rocket",
    "title": "10  프로이센 기병",
    "section": "10.3 런던 투하 V2 로켓",
    "text": "10.3 런던 투하 V2 로켓\n2차 세계대전 중 독일은 신형 무기 V1, V2 로켓을 개발하여 전쟁 막판에 영국 런던을 폭격하여 반전을 노렸다. 독일 신형폭탄의 공격을 받은 영국에서는 독일에서 발사한 신형 폭탄이 정밀 타격한 것인지 아니면 무작위로 대충 발사를 한 것인지 데이터를 통해 검정을 하고자 한다. 1 2\n\n10.3.1 데이터\n가장 먼저 데이터를 준비한다. 데이터는 R.D. Clarke, “An Applicatin of the Poisson Distribution”을 참조한다.\n\n# 1. 기본 데이터 --------- \n\nbombs &lt;- c(\"0 개\",\"1 개\", \"2 개\",\"3 개\",\"4 개\",\"5 개 이상\")\nhit &lt;- c(229, 211, 93, 35, 7, 1)\nexpected &lt;- c(226.74, 211.39, 98.54, 30.62, 7.14, 1.57)\n\nbomb_df &lt;- tibble(bombs, hit, expected)\nbomb_df |&gt; \n  gt() |&gt; \n  gt_theme_538()\n\n\n\n\n\n\n\nbombs\nhit\nexpected\n\n\n\n\n0 개\n229\n226.74\n\n\n1 개\n211\n211.39\n\n\n2 개\n93\n98.54\n\n\n3 개\n35\n30.62\n\n\n4 개\n7\n7.14\n\n\n5 개 이상\n1\n1.57\n\n\n\n\n\n\n\n\n\n10.3.2 포아송 분포\n런던에 떨어진 폭탄이 포아송 분포, 즉 무작위로 떨어진 것이라고 가정하고 시각화를 한다. 포아송 분포는 모수가 \\(\\lambda\\) 하나만 추정하면 되기 때문에 데이터에서 모수를 추정한다.\n\\[P(\\text{ 해당 구간에서 발생한 k개 사건(k events in interval)}) = e^{-\\lambda}\\frac{\\lambda^k}{k!}\\]\n\n# 2. 포아송 분포 --------- \n\nhit &lt;- 537\narea &lt;- 576\n\n(lambda &lt;- hit/area)\n#&gt; [1] 0.9322917\n\nggplot(bomb_df, aes(x=bombs,xend=bombs, y=0, yend=hit)) +\n  geom_segment(size=1.5) +\n  geom_point(aes(bombs, expected), size=2, color=\"red\") +\n  labs(x=\"런던 지역에 투하된 폭탄 수\", y=\"런던 지역 숫자\", title=\"영국 런던에 떨어진 V2 로켓 폭탄\",\n       subtitle=\"실제 투하 폭탄수와 포아송 분포로 추정한 폭탄수\")\n\n\n\n\n\n\n\n\n모수(\\(\\lambda\\))는 0.9322917로 추정된다. 이를 실제 데이터와 포아송 분포에서 나온 데이터와 겹쳐 시각화한다.\n예를 들어, 폭탄이 투하되지 않을 확률은 다음과 같다.\n\\[P(x=0) = e^{-0.9322917}\\frac{0.9322917^0}{0!} = 0.3936506\\]\n이를 R 코드로 표현하면 다음과 같다.\n\nlambda^0 *exp(-lambda) / factorial(0)\n#&gt; [1] 0.3936506\n\n\n\n10.3.3 가설 검정\n시각적으로 살펴봤지만, 통계적 가설검정을 통해 다시 한번 런던에 투척된 폭탄이 포아송 분포를 따르는 것인지 검정해본다.\n\n귀무가설(\\(H_0\\)): 런던에 투하된 폭탄은 무작위로 떨어진 것이다. 즉, 폭탄이 떨어진 분포는 포아송 분포다.\n대립가설(\\(H_A\\)): 폭탄이 떨어진 것은 의도를 갖고 특정지역에 투하된 것이다.\n\n유의수준을 설정하고 검정통계량 \\(\\chi^2\\)을 정의해서 계산하면 귀무가설을 채택하게 된다.\n\n# 3. 통계적 검정 --------- \n\nchisq.test(bomb_df$hit, p=bomb_df$expected, rescale.p=TRUE, simulate.p.value=TRUE)\n#&gt; \n#&gt;  Chi-squared test for given probabilities with simulated p-value (based\n#&gt;  on 2000 replicates)\n#&gt; \n#&gt; data:  bomb_df$hit\n#&gt; X-squared = 1.1709, df = NA, p-value = 0.945\n\n# 4. 최종 데이터 ---------\n\nbomb_df$r_expected &lt;- 573 * c( dpois(0:4, lambda), 1 - sum(dpois(0:4, lambda)))\n\nbomb_df |&gt; \n  gt() |&gt; \n    gt_theme_538()\n\n\n\n\n\n\n\nbombs\nhit\nexpected\nr_expected\n\n\n\n\n0 개\n229\n226.74\n225.561771\n\n\n1 개\n211\n211.39\n210.289359\n\n\n2 개\n93\n98.54\n98.025509\n\n\n3 개\n35\n30.62\n30.462788\n\n\n4 개\n7\n7.14\n7.100051\n\n\n5 개 이상\n1\n1.57\n1.560522\n\n\n\n\n\n\n\n\n\n10.3.4 지리정보를 통한 이해\n공간정보를 활용한 사례로 이를 공간정보에 시각화하면 다음과 같다. 물론 정확한 데이터가 없어 런던 남부에 떨어진 폭탄이 포아송 분포를 따른다고 가정하고 576개 구획으로 나눈 것에 임의로 폭탄이 떨어진 것을 시각화하면 다음과 같다.\n\n# 5. 지리정보 ---------\nlibrary(spatstat)\npar(mar = rep(0, 4))\n\n# 24*24 = 576\nsouth_london &lt;- rpoispp(lambda, win = owin(c(0, 24), c(0, 24)))\nplot(south_london, main=\"\", cex=0.5)\nabline(h = 0:24, v = 0:24, col = \"lightgray\", lty = 3)\n\n\n\n\n\n\n\n\n포아송 분포를 가정하고 통계적 검정도 물론 가능하다. spatstat 팩키지의 함수를 활용하여 통계적 검정을 해도 동일한 결론에 도달하게 된다.\n\nbomb_test &lt;- quadrat.test(south_london, nx = 24, ny = 24, method=\"Chisq\")\nbomb_test\n#&gt; \n#&gt;  Chi-squared test of CSR using quadrat counts\n#&gt; \n#&gt; data:  south_london\n#&gt; X2 = 559, df = 575, p-value = 0.648\n#&gt; alternative hypothesis: two.sided\n#&gt; \n#&gt; Quadrats: 24 by 24 grid of tiles\n#&gt; Tessellation is marked",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>프로이센 기병</span>"
    ]
  },
  {
    "objectID": "cs_prussia.html#footnotes",
    "href": "cs_prussia.html#footnotes",
    "title": "10  프로이센 기병",
    "section": "",
    "text": "R. D. Clarke, “An Application of the Poisson Distribution,” Journal of the Institute of Actuaries, Vol. 72 (1946), p. 481; V-2↩︎\nGeneralized Linear Models - 4. Poisson Models for Count Data↩︎",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>프로이센 기병</span>"
    ]
  },
  {
    "objectID": "cs_german_tank.html",
    "href": "cs_german_tank.html",
    "title": "5  독일 탱크 문제",
    "section": "",
    "text": "5.1 중요성\n제2차 세계대전에서 독일 탱크 생산대수의 정확한 추정은 연합국의 전략적, 전술적 결정 및 심리적 전쟁을 주도하는 데 결정적인 역할을 했다.\n독일의 탱크 생산능력은 그들의 군사 능력과 전쟁 지속 능력을 나타내는 중요한 지표였다. 이를 통해 연합국은 한정된 자원을 어떻게 효율적으로 사용할지 결정할 수 있었다. 만약 독일이 대규모의 탱크를 적극적으로 생산 중이라는 정보를 획득하면, 연합국은 빠르게 자신들의 탱크 생산을 증대시키거나 대탱크 무기의 개발에 투자하는 등의 전략적 대응을 준비했다.\n탱크 생산대수와 탱크종류는 독일군이 다음에 취할 전략과 전술을 예측하는 데 귀중한 정보를 제공했다. 이러한 정보는 전장에서의 우위를 확보하고 적의 움직임을 선제적으로 차단하는 데 도움을 줬고, 심리적 측면에서도 탱크의 생산 정보는 중요했다. 독일의 실제 탱크 생산 능력을 과소 평가하거나 과대 평가하는 것은 연합국의 사기와 전략에 큰 영향을 미쳤다.\n전쟁이 종료된 이후에도, 독일의 탱크 및 기타 군사 장비의 생산 정보는 평화 협정의 조건을 결정하는 데 중요한 자료로 활용되었으며, 독일의 잠재적 군사 능력을 평가하고 장래의 안전 보장을 위한 조치를 결정하는 데 필수적인 정보자산이 되었다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>독일 탱크 문제</span>"
    ]
  },
  {
    "objectID": "cs_german_tank.html#데이터-수집",
    "href": "cs_german_tank.html#데이터-수집",
    "title": "5  독일 탱크 문제",
    "section": "5.2 데이터 수집",
    "text": "5.2 데이터 수집\n제2차 세계대전 중 독일 탱크의 시리얼 번호 획득은 연합국의 미묘한 정보 수집 작업 중 하나였다. 전투 중 파괴되거나 포획된 독일 탱크들은 대부분 고유의 시리얼 번호가 달려있었으며, 시리얼 번호는 탱크 생산 순서와 일련번호를 나타냈기 때문에, 연합국은 이러한 번호를 꼼꼼히 수집했다. 파손된 탱크 부품에서도 이러한 번호를 발견할 수 있었고, 이를 통해 전체 탱크 생산 규모의 일부를 엿볼 수 있었다. 이런 정보들은 전쟁 중 독일의 탱크 생산 능력을 추정하는 데 매우 중요한 열쇠로 작용했다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>독일 탱크 문제</span>"
    ]
  },
  {
    "objectID": "cs_german_tank.html#월별-탱크생산대수",
    "href": "cs_german_tank.html#월별-탱크생산대수",
    "title": "5  독일 탱크 문제",
    "section": "5.3 월별 탱크생산대수",
    "text": "5.3 월별 탱크생산대수\n제2차 세계대전 기간 독일 탱크의 생산 대수를 분석한 표에 따르면, 다양한 추정치와 실제 생산된 탱크의 수를 비교할 수 있다. 1940년 6월에는 통계적 방법을 통해 169대의 탱크가 생산되었다고 추정되었으나, 정보기관은 1,000대로 추정했다. 그러나 실제로는 122대만 생산되었다. 이어서 1941년 6월에는 통계적으로는 244대, 정보기관에서는 1,550대로 추정되었고, 실제로는 271대가 생산되었다. 마지막으로 1942년 8월에는 통계적 추정치가 327대, 정보기관의 추정치가 1,550대로 나왔으며, 실제 생산된 탱크는 342대였다. 이를 통해 보면, 통계적 방법으로 추정한 생산 대수가 정보기관의 추정보다 실제 생산량에 더 가까웠음을 알 수 있다.\n\n\n\n년월\n통계 추정\n정보기관 추정\n독일 생산대수\n\n\n\n\n1940년 6월\n169\n1,000\n122\n\n\n1941년 6월\n244\n1,550\n271\n\n\n1942년 8월\n327\n1,550\n342",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>독일 탱크 문제</span>"
    ]
  },
  {
    "objectID": "cs_german_tank.html#통계-추정방법",
    "href": "cs_german_tank.html#통계-추정방법",
    "title": "5  독일 탱크 문제",
    "section": "5.4 통계 추정방법",
    "text": "5.4 통계 추정방법\nMLE를 사용하여 독일 탱크의 총 생산 수를 추정하는 방법은 관측된 탱크의 일련 번호 간의 평균 간격을 기준으로 한다.\n예를 들어, 탱크 \\(k\\) 대를 관측했을 때, 가장 큰 일련 번호가 \\(m\\)이면, 평균적으로 탱크 사이의 간격은 \\(\\frac{m}{k}\\)이다. 이 간격을 기반으로, 번호 \\(m\\) 이후에 아직 관측되지 않은 탱크는 대략 \\(\\frac{m}{k}\\) 대가 더 있을 것으로 예상된다.\n이 정보를 토대로, 전체 탱크 수에 대한 추정치 \\(N\\)는 다음의 공식으로 구할 수 있다:\n\\[\nN_{\\text{추정치}} = m + \\frac{m}{k} - 1\n\\]\n독일군에서 5대의 탱크를 포획하였다고 상상해보자. 포획한 탱크의 일련 번호는 2, 5, 7, 20, 그리고 31이다. 여기서, 최대 일련 번호 \\(m\\)은 31이고, 포획된 탱크의 총 수 \\(k\\)는 5대이다.\n이 정보를 사용하여 탱크의 총 생산 수 \\(N\\)을 추정하면:\n\\[\nN_{\\text{추정치}} = m + \\frac{m}{k} - 1 = 31 + \\frac{31}{5} - 1\n\\]\n따라서, \\(N_{\\text{추정}}\\)의 값은 36.2로, 총 탱크 수는 대략 36 또는 37대로 추정될 수 있다.",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>독일 탱크 문제</span>"
    ]
  },
  {
    "objectID": "cs_bomber.html",
    "href": "cs_bomber.html",
    "title": "12  폭격비행기",
    "section": "",
    "text": "12.1 세계대전",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>폭격비행기</span>"
    ]
  },
  {
    "objectID": "cs_bomber.html#세계대전",
    "href": "cs_bomber.html#세계대전",
    "title": "12  폭격비행기",
    "section": "",
    "text": "12.1.1 유럽전선\n\n\n\n12.1.2 태평양 전선",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>폭격비행기</span>"
    ]
  },
  {
    "objectID": "cs_bomber.html#대공포",
    "href": "cs_bomber.html#대공포",
    "title": "12  폭격비행기",
    "section": "12.2 대공포",
    "text": "12.2 대공포",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>폭격비행기</span>"
    ]
  },
  {
    "objectID": "cs_bomber.html#폭격기-데이터",
    "href": "cs_bomber.html#폭격기-데이터",
    "title": "12  폭격비행기",
    "section": "12.3 폭격기 데이터",
    "text": "12.3 폭격기 데이터\n생존자 편향(Survivorship Bias)의 오류는 실패한 사례나 숨겨진 요인을 고려하지 않고 오직 ‘생존’ 혹은 ‘성공’ 사례만을 분석하여 일반화하는 과정에서 발생하여 데이터나 결과 해석을 왜곡시킬 수 있다.\n아브라함 왈드(Abraham Wald)는 제2차 세계대전 동안 컬럼비아 대학 통계연구그룹(Statistical Research Group)에서 일했다. 전투기와 같은 군사 장비가 적으로부터 얼마나 많은 피해를 입을 수 있는지에 대한 분석을 수행했으며, 특히 ‘생존자 편향(Survivorship Bias)’ 문제를 고려한 통계적 방법을 제안하여 중요한 기여를 했다.\n전투기가 전투에서 돌아올 때 어떤 부분이 가장 많이 손상되는지에 대한 데이터가 있었다. 대부분의 사람들은 데이터를 보고 손상이 가장 많이 발생한 부분을 강화해야 한다고 생각했지만, 왈드는 이와는 반대의 접근을 택했는데, 돌아오지 못한 전투기에 대한 데이터가 누락되어 있으므로, 이를 고려하지 않으면 편향된 결론을 내릴 수 있다고 지적했다. 즉, 손상이 적게 발생한 부분이 전투기 귀환에 오히려 더 중요할 수 있다는 것이다.\n\n\n\n귀환한 폭격기 피탄 위치\n\n\n전폭기를 크게 4가지 부분으로 나눠 데이터를 임의로 생성해보자.\n\nWing_Hits : 전폭기 날개\nFuselage_Hits : 전폭기 주 몸체\nEngine_Hits : 전폭기 엔진\nCockpit_Hits : 전폭기 조종석\n\n\n# tidyverse 패키지 불러오기\nlibrary(tidyverse)\n\n# 난수 생성을 위한 seed 설정\nset.seed(123)\n\n# 100대의 전투기 데이터 생성\nn_planes &lt;- 100\n\n# 데이터프레임 생성\nplane_raw &lt;- tibble(\n  Plane_ID = 1:n_planes,\n  Wing_Hits = sample(0:20, n_planes, replace = TRUE),\n  Fuselage_Hits = sample(0:10, n_planes, replace = TRUE),\n  Engine_Hits = sample(0:5, n_planes, replace = TRUE),\n  Cockpit_Hits = sample(0:2, n_planes, replace = TRUE)\n) \n  \nplane_tbl &lt;- plane_raw |&gt; \n  pivot_longer(-Plane_ID, names_to = \"피탄부위\", values_to = \"피탄수\") |&gt; \n  mutate(부위 = case_match(피탄부위,\n                         \"Wing_Hits\"     ~ \"날개\",\n                         \"Fuselage_Hits\" ~ \"주 몸체\",\n                         \"Engine_Hits\"   ~ \"엔진\",\n                         \"Cockpit_Hits\"  ~ \"조종석\"))\n\n# 각 부위별 총 타격 수 계산\nsummary_tbl &lt;- plane_tbl %&gt;% \n  group_by(피탄부위, 부위) |&gt; \n  summarise(총피탄수 = sum(피탄수)) |&gt; \n  arrange(desc(총피탄수))\n\nsummary_tbl\n# 계산 결과 확인\n\nair_gg &lt;- summary_tbl |&gt; \n  ggplot(aes(x = fct_reorder(부위, 총피탄수), y = 총피탄수)) +\n    geom_col(width = 0.3) +\n    coord_flip() +\n    labs(x=\"\",\n         title = \"제2차 세계대전 폭격기 부위별 피탄수\") +\n    theme_minimal() +\n    scale_y_continuous(labels = scales::comma)\n\nragg::agg_jpeg(\"images/air_gg.jpg\",\n              width = 10, height = 7, units = \"in\", res = 600)\nair_gg\ndev.off()",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>폭격비행기</span>"
    ]
  },
  {
    "objectID": "cs_bomber.html#보강",
    "href": "cs_bomber.html#보강",
    "title": "12  폭격비행기",
    "section": "12.4 보강",
    "text": "12.4 보강\n날개(Wing) 부분이 가장 많이 타격을 받아 이 부분과 그 다음 주 몸체를 보강해야 된다고 볼 수도 있다. 이는 생존한 전투기만 고려한 전형적인 생존자 편향으로 실제로 엔진과 조종석을 보강해야 더 큰 효과를 볼 수 있다.\n\n\n\n폭격기 보강 전략",
    "crumbs": [
      "사례연구",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>폭격비행기</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "참고문헌",
    "section": "",
    "text": "Hendron, Kaden. 2016. “Germany’s u-Boats & Data Visualization:\nCan Data Visualization Offer Answers to Our Questions about\nHistory?” April 20, 2016. https://medium.com/@kadenhendron/germany-s-u-boats-data-visualization-6e018c6c174.\n\n\nMarwick, Ben, Carl Boettiger, and Lincoln Mullen. 2018. “Packaging\nData Analytical Work Reproducibly Using r (and Friends).” The\nAmerican Statistician 72 (1): 80–88.\n\n\nStevens, Stanley Smith. 1946. “On the Theory of Scales of\nMeasurement.” Science 103 (2684): 677–80. http://www.jstor.org/stable/1671815.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\".\n\n\nWiener, Norbert. 1921. “A New Theory of Measurement: A Study in\nthe Logic of Mathematics.” Proceedings of the London\nMathematical Society 2 (1): 181–205.\n\n\n이경화. 2020. 고등학교 실용통계. 통계청 통계교육원.",
    "crumbs": [
      "참고문헌"
    ]
  }
]